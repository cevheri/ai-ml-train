{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Computer Vision With Deep Learning Part1",
   "id": "e3f8124173d50189"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "* In this notebook, we will learn about computer vision and deep learning. \n",
    "* We will start by understanding the basics of computer vision and then move on to deep learning.\n",
    "* We will also learn about convolutional neural networks (CNNs) and how they are used in computer vision.\n",
    "* We will implement a simple CNN model using the Keras library and train it on the MNIST dataset.\n",
    "* Finally, we will evaluate the model and visualize the results."
   ],
   "id": "318be79cf6722f70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Install tensorflow\")\n",
    "# !pip install tensorflow"
   ],
   "id": "43450c503f688763"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is Computer Vision?\n",
    "Computer vision is a field of artificial intelligence that enables computers to interpret and understand the visual world. \n"
   ],
   "id": "95dc201c9c59f62a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Our Goals\n",
    "* Understand the basics of computer vision and deep learning.\n",
    "* Detect and recognize objects in images using deep learning.\n",
    "* Learn about convolutional neural networks (CNNs) and how they are used in computer vision.\n",
    "* Implement a simple CNN model using the Keras library and train it on the MNIST dataset.\n",
    "* Evaluate the model and visualize the results.\n",
    "* Learn about techniques to improve the model's performance on image classification tasks.\n",
    "* Save the model for future use."
   ],
   "id": "2aeeaa296a20dd65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Libraries",
   "id": "bb9f336e5519c68a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.024858Z",
     "start_time": "2024-07-27T15:41:53.013324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "f6e2e55d414705e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 18:41:53.164749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-27 18:41:53.174458: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-27 18:41:53.177650: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Handwritten Digit Recognition\n",
    "\n",
    "Download 60000 images of handwritten digits from the MNIST dataset."
   ],
   "id": "c7b7577c6aa9a291"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.165297Z",
     "start_time": "2024-07-27T15:41:54.025816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist = tf.keras.datasets.mnist # 28x28 images of hand-written digits 0-9\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# df = tf.keras.utils.image_dataset_from_directory(\"images/trafic-lights\")\n"
   ],
   "id": "bfbf58a56f0d02e4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.169405Z",
     "start_time": "2024-07-27T15:41:54.166035Z"
    }
   },
   "cell_type": "code",
   "source": "train_images.shape\n",
   "id": "b71792fb32effc85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.279457Z",
     "start_time": "2024-07-27T15:41:54.170235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.imshow(train_images[0], cmap='gray')"
   ],
   "id": "105fd0a08931ea85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f52b303c890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.282768Z",
     "start_time": "2024-07-27T15:41:54.280378Z"
    }
   },
   "cell_type": "code",
   "source": "train_labels[0]",
   "id": "9c77820d1a94c2a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.293875Z",
     "start_time": "2024-07-27T15:41:54.283199Z"
    }
   },
   "cell_type": "code",
   "source": "test_images.shape",
   "id": "95351e028398c4b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.303837Z",
     "start_time": "2024-07-27T15:41:54.294380Z"
    }
   },
   "cell_type": "code",
   "source": "# train_images[61]",
   "id": "24fa15b191936175",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Normalizing the data\n",
    "When dividing by 255, we are normalizing the data. \n",
    "This is because the pixel values range from 0 to 255. \n",
    "By dividing by 255, we are scaling the pixel values to be between 0 and 1.\n",
    "\n",
    "### Normalize formula\n",
    "\n",
    "<img src=\"Normalization-Formula.jpg\" width=\"400\">"
   ],
   "id": "3799fe4808efdddf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question:\n",
    "Why do we divide by 255?\n",
    "### Answer: \n",
    "We divide by 255 to normalize the pixel values. The pixel values range from 0 to 255, and by dividing by 255, we are scaling the pixel values to be between 0 and 1. This helps the model learn better and faster."
   ],
   "id": "343ad83b0d18eb41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.448278Z",
     "start_time": "2024-07-27T15:41:54.304325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Normalization formula for pixel values\")\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "print(\"Normalized pixel values is same as original pixel values\")"
   ],
   "id": "1a89f81eee63bbc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization formula for pixel values\n",
      "Normalized pixel values is same as original pixel values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network - CNN - Image Classification\n",
    "Explanation of CNN\n",
    "\n",
    "### \n",
    "<img src=\"cnn1.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "### \n",
    "<img src=\"cnn2.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "###  \n",
    "<img src=\"cnn3.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "###  \n",
    "<img src=\"cnn4.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "###  \n",
    "<img src=\"cnn5.png\" width=\"500\">\n"
   ],
   "id": "baac5903a258944d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.451419Z",
     "start_time": "2024-07-27T15:41:54.448907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, InputLayer, Reshape, MaxPooling2D, Flatten"
   ],
   "id": "7c6f03af3d105e72",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building the CNN model with Bird Images\n",
    "\n",
    "### Step 1: Load the images\n",
    "### Step 2: Preprocess the images\n",
    "### Step 3: Build the CNN model\n",
    "### Step 4: Compile the model\n",
    "### Step 5: Train the model\n",
    "### Step 6: Evaluate the model\n",
    "\n",
    "<img src=\"bird-cnn.png\" width=\"500\">"
   ],
   "id": "41fcad849733d8d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.590469Z",
     "start_time": "2024-07-27T15:41:54.452578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "# 12 filters, each filter is 3x3. Q:What is kernel size? A: The kernel size is the size of the filter that is applied to the input image. \n",
    "# In this case, the kernel size is 3x3, which means that the filter is a 3x3 matrix that is applied to the input image.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ],
   "id": "5c385c5cc9733e71",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722094914.535671  498306 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:24.876515Z",
     "start_time": "2024-07-27T15:41:54.591304Z"
    }
   },
   "cell_type": "code",
   "source": "history = model.fit(train_images, train_labels, validation_split=0.20, epochs=10)",
   "id": "f2beb57ae403a192",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.8440 - loss: 0.5591 - val_accuracy: 0.9560 - val_loss: 0.1598\n",
      "Epoch 2/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9597 - loss: 0.1398 - val_accuracy: 0.9722 - val_loss: 0.1041\n",
      "Epoch 3/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9733 - loss: 0.0950 - val_accuracy: 0.9758 - val_loss: 0.0878\n",
      "Epoch 4/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9776 - loss: 0.0753 - val_accuracy: 0.9750 - val_loss: 0.0855\n",
      "Epoch 5/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9814 - loss: 0.0643 - val_accuracy: 0.9776 - val_loss: 0.0794\n",
      "Epoch 6/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9852 - loss: 0.0513 - val_accuracy: 0.9774 - val_loss: 0.0754\n",
      "Epoch 7/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9864 - loss: 0.0469 - val_accuracy: 0.9793 - val_loss: 0.0716\n",
      "Epoch 8/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9869 - loss: 0.0432 - val_accuracy: 0.9812 - val_loss: 0.0688\n",
      "Epoch 9/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9884 - loss: 0.0371 - val_accuracy: 0.9812 - val_loss: 0.0671\n",
      "Epoch 10/10\n",
      "\u001B[1m1500/1500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9909 - loss: 0.0340 - val_accuracy: 0.9816 - val_loss: 0.0668\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.165262Z",
     "start_time": "2024-07-27T15:42:24.877279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "loss, accuracy"
   ],
   "id": "75c7eef65c855a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 603us/step - accuracy: 0.9786 - loss: 0.0727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06021083891391754, 0.9817000031471252)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model for future use",
   "id": "4d52d5b089a82246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.183802Z",
     "start_time": "2024-07-27T15:42:25.165789Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"my-handwritten-recognition-model.keras\")",
   "id": "ef990895ac5e6208",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Improve the model\n",
    "\n",
    "### Technique1: Increase the number of layers"
   ],
   "id": "62431a42c3bc7e2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.212658Z",
     "start_time": "2024-07-27T15:42:25.184407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ],
   "id": "a2f22a9b74c2822d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Technique2: Increase the number of neurons",
   "id": "1d3cbb7712015190"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.234117Z",
     "start_time": "2024-07-27T15:42:25.213383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ],
   "id": "ee86e472eac751ad",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Technique3: add dropout layer to prevent overfitting",
   "id": "38c5842ab77636f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.257053Z",
     "start_time": "2024-07-27T15:42:25.234657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dropout(0.5)) # add a dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ],
   "id": "8c96712acb6cb878",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Technique4: Batch normalization",
   "id": "16ef91a5896e9a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.287240Z",
     "start_time": "2024-07-27T15:42:25.257609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(BatchNormalization()) # add a batch normalization layer\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dropout(0.25)) # add a dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ],
   "id": "61ba92c221d050ce",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Appendix\n",
    "### Search and Download Images Using Bing Image Downloader"
   ],
   "id": "6691d681dbd93680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Install python-bing-image-downloader\")\n",
    "## install bing-image-downloader\n",
    "# !pip install bing-image-downloader \n",
    "print(\"Image download\")\n",
    "from bing_image_downloader import downloader\n",
    "downloader.download(\"bird\", limit=1, output_dir='images', adult_filter_off=True)\n",
    "downloader.download(\"skincancer\", limit=1, output_dir='images', adult_filter_off=True)"
   ],
   "id": "a4bba594e0f06cfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Deep learning with TensorFlow and Keras is a powerful tool for image classification tasks.\n",
    "* In this notebook, we learned about computer vision and deep learning.\n",
    "* Also, we learned about convolutional neural networks (CNNs) and how they are used in computer vision.\n",
    "* We implemented a simple CNN model using the Keras library and trained it on the MNIST dataset.\n",
    "* Finally, we evaluated the model and visualized the results.\n",
    "* We also learned about techniques to improve the model, such as increasing the number of layers, neurons, adding dropout layers, and batch normalization.\n",
    "* We can use these techniques to improve the model's performance on image classification tasks.\n",
    "* We also learned how to save the model for future use.\n",
    "* Bonus: We also learned how to download images using the Bing Image Downloader library. :)"
   ],
   "id": "c9dab62afb9c96e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "* https://www.tensorflow.org\n",
    "* https://keras.io\n",
    "* https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "* https://www.youtube.com/watch?v=wIF0AOqIhPM\n",
    "* https://www.deeplearningbook.org/\n",
    "* Thanks Zafer Acar"
   ],
   "id": "b217b9a610f3c4b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Full source code\n",
    "[Computer Vision With Deep Learning Part1](https://github.com/cevheri/ai-ml-train/blob/main/Day%2009/Day09%20-%20Computer%20Vision%20With%20Deep%20Learning.ipynb)"
   ],
   "id": "446018125f32ccaa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
