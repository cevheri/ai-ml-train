{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f8124173d50189",
   "metadata": {},
   "source": [
    "# Computer Vision With Deep Learning Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318be79cf6722f70",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "* In this notebook, we will learn about computer vision and deep learning. \n",
    "* We will start by understanding the basics of computer vision and then move on to deep learning.\n",
    "* We will also learn about convolutional neural networks (CNNs) and how they are used in computer vision.\n",
    "* We will implement a simple CNN model using the Keras library and train it on the MNIST dataset.\n",
    "* Finally, we will evaluate the model and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43450c503f688763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install tensorflow\n"
     ]
    }
   ],
   "source": [
    "print(\"Install tensorflow\")\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc201c9c59f62a",
   "metadata": {},
   "source": [
    "## What is Computer Vision?\n",
    "Computer vision is a field of artificial intelligence that enables computers to interpret and understand the visual world. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeeaa296a20dd65",
   "metadata": {},
   "source": [
    "## Our Goals\n",
    "* Understand the basics of computer vision and deep learning.\n",
    "* Detect and recognize objects in images using deep learning.\n",
    "* Learn about convolutional neural networks (CNNs) and how they are used in computer vision.\n",
    "* Implement a simple CNN model using the Keras library and train it on the MNIST dataset.\n",
    "* Evaluate the model and visualize the results.\n",
    "* Learn about techniques to improve the model's performance on image classification tasks.\n",
    "* Save the model for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f336e5519c68a",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e2e55d414705e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.024858Z",
     "start_time": "2024-07-27T15:41:53.013324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 19:05:25.111352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-27 19:05:25.121080: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-27 19:05:25.124035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7577c6aa9a291",
   "metadata": {},
   "source": [
    "## Handwritten Digit Recognition\n",
    "\n",
    "Download 60000 images of handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbf58a56f0d02e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.165297Z",
     "start_time": "2024-07-27T15:41:54.025816Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist # 28x28 images of hand-written digits 0-9\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# df = tf.keras.utils.image_dataset_from_directory(\"images/trafic-lights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71792fb32effc85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.169405Z",
     "start_time": "2024-07-27T15:41:54.166035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "105fd0a08931ea85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.279457Z",
     "start_time": "2024-07-27T15:41:54.170235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcd4dab7b90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.imshow(train_images[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c77820d1a94c2a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.282768Z",
     "start_time": "2024-07-27T15:41:54.280378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95351e028398c4b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.293875Z",
     "start_time": "2024-07-27T15:41:54.283199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24fa15b191936175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.303837Z",
     "start_time": "2024-07-27T15:41:54.294380Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_images[61]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799fe4808efdddf",
   "metadata": {},
   "source": [
    "## Normalizing the data\n",
    "When dividing by 255, we are normalizing the data. \n",
    "This is because the pixel values range from 0 to 255. \n",
    "By dividing by 255, we are scaling the pixel values to be between 0 and 1.\n",
    "\n",
    "### Normalize formula\n",
    "\n",
    "<img src=\"Normalization-Formula.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ad83b0d18eb41",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Why do we divide by 255?\n",
    "### Answer: \n",
    "We divide by 255 to normalize the pixel values. The pixel values range from 0 to 255, and by dividing by 255, we are scaling the pixel values to be between 0 and 1. This helps the model learn better and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a89f81eee63bbc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.448278Z",
     "start_time": "2024-07-27T15:41:54.304325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization formula for pixel values\n",
      "Normalized pixel values is same as original pixel values\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Normalization formula for pixel values\")\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "print(\"Normalized pixel values is same as original pixel values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac5903a258944d",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network - CNN - Image Classification\n",
    "Explanation of CNN\n",
    "\n",
    "### \n",
    "<img src=\"cnn1.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "### \n",
    "<img src=\"cnn2.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "###  \n",
    "<img src=\"cnn3.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "###  \n",
    "<img src=\"cnn4.png\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "###  \n",
    "<img src=\"cnn5.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c6f03af3d105e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.451419Z",
     "start_time": "2024-07-27T15:41:54.448907Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, InputLayer, Reshape, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fcad849733d8d6",
   "metadata": {},
   "source": [
    "## Building the CNN model with Bird Images\n",
    "\n",
    "### Step 1: Load the images\n",
    "### Step 2: Preprocess the images\n",
    "### Step 3: Build the CNN model\n",
    "### Step 4: Compile the model\n",
    "### Step 5: Train the model\n",
    "### Step 6: Evaluate the model\n",
    "\n",
    "<img src=\"bird-cnn.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c385c5cc9733e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:41:54.590469Z",
     "start_time": "2024-07-27T15:41:54.452578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722096326.616209  507574 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "# 12 filters, each filter is 3x3. Q:What is kernel size? A: The kernel size is the size of the filter that is applied to the input image. \n",
    "# In this case, the kernel size is 3x3, which means that the filter is a 3x3 matrix that is applied to the input image.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2beb57ae403a192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:24.876515Z",
     "start_time": "2024-07-27T15:41:54.591304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8522 - loss: 0.5513 - val_accuracy: 0.9655 - val_loss: 0.1270\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1132 - val_accuracy: 0.9733 - val_loss: 0.0986\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.0820 - val_accuracy: 0.9768 - val_loss: 0.0827\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0613 - val_accuracy: 0.9784 - val_loss: 0.0753\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0539 - val_accuracy: 0.9806 - val_loss: 0.0689\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0460 - val_accuracy: 0.9810 - val_loss: 0.0703\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0423 - val_accuracy: 0.9809 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0379 - val_accuracy: 0.9812 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0361 - val_accuracy: 0.9788 - val_loss: 0.0713\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0307 - val_accuracy: 0.9807 - val_loss: 0.0695\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, validation_split=0.20, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75c7eef65c855a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.165262Z",
     "start_time": "2024-07-27T15:42:24.877279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.9782 - loss: 0.0740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06176856532692909, 0.9819999933242798)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52d5b089a82246",
   "metadata": {},
   "source": [
    "## Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef990895ac5e6208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.183802Z",
     "start_time": "2024-07-27T15:42:25.165789Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"my-handwritten-recognition-model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62431a42c3bc7e2c",
   "metadata": {},
   "source": [
    "## Improve the model\n",
    "\n",
    "### Technique1: Increase the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f22a9b74c2822d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.212658Z",
     "start_time": "2024-07-27T15:42:25.184407Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cbb7712015190",
   "metadata": {},
   "source": [
    "### Technique2: Increase the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee86e472eac751ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.234117Z",
     "start_time": "2024-07-27T15:42:25.213383Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5842ab77636f3",
   "metadata": {},
   "source": [
    "### Technique3: add dropout layer to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c96712acb6cb878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.257053Z",
     "start_time": "2024-07-27T15:42:25.234657Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dropout(0.5)) # add a dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef91a5896e9a3b",
   "metadata": {},
   "source": [
    "### Technique4: Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61ba92c221d050ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:42:25.287240Z",
     "start_time": "2024-07-27T15:42:25.257609Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(28, 28))) # images are 28x28 pixels\n",
    "model.add(Reshape(target_shape=(28, 28, 1))) # reshape the images to 28x28x1\n",
    "model.add(Conv2D(filters=12, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # max pooling with a pool size of 2x2\n",
    "model.add(Flatten()) # flatten the output of the convolutional layers\n",
    "model.add(BatchNormalization()) # add a batch normalization layer\n",
    "model.add(Dense(units=128)) # add a dense layer with 128 units and relu activation\n",
    "model.add(Dropout(0.25)) # add a dropout layer with a dropout rate of 0.5\n",
    "model.add(Dense(units=10)) # output layer with 10 units (one for each digit) \n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691d681dbd93680",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "### Search and Download Images Using Bing Image Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4bba594e0f06cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install python-bing-image-downloader\n",
      "Image download\n",
      "[%] Downloading Images to /home/cevheri/Documents/ai/Day 09/images/bird\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 61 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://au.newhollandpublishers.com/pub/media/catalog/product/cache/32c3db8536f26a5ade98b0e2d5530502/u/r/urban_birds_working_cover.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 1 images.\n",
      "[%] Downloading Images to /home/cevheri/Documents/ai/Day 09/images/skincancer\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 1 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from https://miiskin.com/wp-content/uploads/2019/08/signs-of-skin-cancer-infographic.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 1 images.\n"
     ]
    }
   ],
   "source": [
    "print(\"Install python-bing-image-downloader\")\n",
    "## install bing-image-downloader\n",
    "# !pip install bing-image-downloader \n",
    "print(\"Image download\")\n",
    "from bing_image_downloader import downloader\n",
    "downloader.download(\"bird\", limit=1, output_dir='images', adult_filter_off=True)\n",
    "downloader.download(\"skincancer\", limit=1, output_dir='images', adult_filter_off=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dab62afb9c96e5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Deep learning with TensorFlow and Keras is a powerful tool for image classification tasks.\n",
    "* In this notebook, we learned about computer vision and deep learning.\n",
    "* Also, we learned about convolutional neural networks (CNNs) and how they are used in computer vision.\n",
    "* We implemented a simple CNN model using the Keras library and trained it on the MNIST dataset.\n",
    "* Finally, we evaluated the model and visualized the results.\n",
    "* We also learned about techniques to improve the model, such as increasing the number of layers, neurons, adding dropout layers, and batch normalization.\n",
    "* We can use these techniques to improve the model's performance on image classification tasks.\n",
    "* We also learned how to save the model for future use.\n",
    "* Bonus: We also learned how to download images using the Bing Image Downloader library. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217b9a610f3c4b6",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://www.tensorflow.org\n",
    "* https://keras.io\n",
    "* https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "* https://www.youtube.com/watch?v=wIF0AOqIhPM\n",
    "* https://www.deeplearningbook.org/\n",
    "* Thanks Zafer Acar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446018125f32ccaa",
   "metadata": {},
   "source": [
    "## Full source code\n",
    "[Computer Vision With Deep Learning Part1](https://github.com/cevheri/ai-ml-train/blob/main/Day%2009/Day09%20-%20Computer%20Vision%20With%20Deep%20Learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
