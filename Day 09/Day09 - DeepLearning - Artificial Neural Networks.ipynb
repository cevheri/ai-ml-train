{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep Learning - Artificial Neural Networks",
   "id": "2a3136902d1af716"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Deep Learning\n",
    "* Deep learning is a subset of machine learning, which is a subset of artificial intelligence. \n",
    "* \n",
    "\n",
    "## Artificial Neural Networks\n",
    "* Artificial neural networks are computing systems that are inspired by the biological neural networks that constitute animal brains.\n",
    "\n",
    "## number, array, matrix, tensor\n",
    "* number: 5\n",
    "* array: [1, 2, 3, 4, 5]\n",
    "* matrix: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "* tensor: [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]]"
   ],
   "id": "b450ce4aa4483e31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](2024-07-27_09-34.png)",
   "id": "404a51f2cab2cc1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Dictionary\n",
    "* Pregnancies: Number of times pregnant\n",
    "* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "* BloodPressure: Diastolic blood pressure (mm Hg)\n",
    "* SkinThickness: Triceps skin fold thickness (mm)\n",
    "* Insulin: 2-Hour serum insulin (mu U/ml)\n",
    "* BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "* DiabetesPedigreeFunction: Diabetes pedigree function\n",
    "* Age: Age (years)\n",
    "* Outcome: Class variable (0 or 1)\n",
    "\n",
    "Outcome is Y(target) and the rest of the columns are X(features)"
   ],
   "id": "9a83762c16e6474f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install TensorFlow for Deep Learning",
   "id": "81017b2f3ce08f57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:53.091858Z",
     "start_time": "2024-07-27T08:11:53.089936Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install tensorflow",
   "id": "a0200e7872d13686",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing the libraries",
   "id": "2403ce09f93e1059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.367107Z",
     "start_time": "2024-07-27T08:11:53.201137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "id": "f544ccdaa9715f3c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 11:11:53.531391: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-27 11:11:53.540301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-27 11:11:53.551226: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-27 11:11:53.554365: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-27 11:11:53.562496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-27 11:11:54.002596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing the dataset",
   "id": "e6d0fc1ea7ff16b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.371501Z",
     "start_time": "2024-07-27T08:11:54.367977Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('pima-indians-diabetes.csv')",
   "id": "92458d9c9cba5f81",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Show the first 5 rows of the dataset",
   "id": "63da8d0e3cff1629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.396091Z",
     "start_time": "2024-07-27T08:11:54.372079Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "f06a07cd3b5738db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.407360Z",
     "start_time": "2024-07-27T08:11:54.397108Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "78c706209221d672",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.426655Z",
     "start_time": "2024-07-27T08:11:54.408065Z"
    }
   },
   "cell_type": "code",
   "source": "df.describe()",
   "id": "18c19524f5f10a3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.430455Z",
     "start_time": "2024-07-27T08:11:54.427412Z"
    }
   },
   "cell_type": "code",
   "source": "df.isnull().sum()",
   "id": "c860a1e97fab998a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "11595b5e7444ba49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import required libraries",
   "id": "dea07e5056ebd5ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.443983Z",
     "start_time": "2024-07-27T08:11:54.431147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n"
   ],
   "id": "c87741c63d3f68e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create the model",
   "id": "5ee7699c438794ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.463655Z",
     "start_time": "2024-07-27T08:11:54.445019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df.drop('Outcome', axis=1) \n",
    "# or X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "# or X = df.iloc[:,0:8]\n",
    "\n",
    "y = df['Outcome'] # or y = df.iloc[:, 8]\n"
   ],
   "id": "968173c070a75b01",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "6639d629dda93a37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Activation Function\n",
   "id": "f60849bed3ba06ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](activation-func.png)",
   "id": "d2e07f48c66e0df6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](sigmoid.png)",
   "id": "d17b5d15bebf52bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adam Optimizer\n",
    "![](adam.png)"
   ],
   "id": "dc844122d3d6184b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a00ed8b0fdb70e4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.480032Z",
     "start_time": "2024-07-27T08:11:54.464339Z"
    }
   },
   "cell_type": "code",
   "source": "# !python3 -m pip install tensorflow[and-cuda]",
   "id": "3b4a199ddb08904f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.585012Z",
     "start_time": "2024-07-27T08:11:54.481475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) # output layer classification because output is 0 or 1\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# adam: Adaptive Moment Estimation (Adam) is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. We calulate the pass of the street by using the adam optimizer.\n",
    "\n",
    "# binary_crossentropy: binary classification is used for binary classification problems (0 or 1)\n",
    "# metrics=['accuracy']: to see the accuracy score on the training set when we train the model with the training set data and labels (X and y)\n"
   ],
   "id": "46dc9142048d6eed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722067914.536020  280607 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-27 11:11:54.559645: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:11:54.593288Z",
     "start_time": "2024-07-27T08:11:54.585704Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "3f48fcc4edcbec14",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the model\n",
    "* epochs: The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "* batch_size: The batch size is a number of samples processed before the model is updated.\n",
    "  * batch_size= df.shape[0] / 32 = 24\n",
    "* validation_split: Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "  * validation_split=0.2: 20% of the training data is used for validation data. \n",
    "* verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch."
   ],
   "id": "d4fbcb4864dae7d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:15.652426Z",
     "start_time": "2024-07-27T08:11:54.593917Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(X, y, epochs=300, batch_size=10, validation_split=0.2, verbose=1)",
   "id": "302c76b8982bca28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.6537 - loss: 0.6917 - val_accuracy: 0.6429 - val_loss: 0.6877\n",
      "Epoch 2/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6595 - loss: 0.6856 - val_accuracy: 0.6429 - val_loss: 0.6825\n",
      "Epoch 3/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 815us/step - accuracy: 0.6462 - loss: 0.6811 - val_accuracy: 0.6429 - val_loss: 0.6781\n",
      "Epoch 4/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 809us/step - accuracy: 0.6313 - loss: 0.6786 - val_accuracy: 0.6429 - val_loss: 0.6744\n",
      "Epoch 5/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 942us/step - accuracy: 0.6655 - loss: 0.6697 - val_accuracy: 0.6429 - val_loss: 0.6707\n",
      "Epoch 6/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 941us/step - accuracy: 0.6780 - loss: 0.6632 - val_accuracy: 0.6429 - val_loss: 0.6677\n",
      "Epoch 7/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6411 - loss: 0.6676 - val_accuracy: 0.6429 - val_loss: 0.6655\n",
      "Epoch 8/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 781us/step - accuracy: 0.6192 - loss: 0.6707 - val_accuracy: 0.6429 - val_loss: 0.6630\n",
      "Epoch 9/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 799us/step - accuracy: 0.6510 - loss: 0.6603 - val_accuracy: 0.6429 - val_loss: 0.6612\n",
      "Epoch 10/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 758us/step - accuracy: 0.6384 - loss: 0.6622 - val_accuracy: 0.6429 - val_loss: 0.6596\n",
      "Epoch 11/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 820us/step - accuracy: 0.6739 - loss: 0.6489 - val_accuracy: 0.6429 - val_loss: 0.6583\n",
      "Epoch 12/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6331 - loss: 0.6615 - val_accuracy: 0.6429 - val_loss: 0.6571\n",
      "Epoch 13/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6584 - loss: 0.6511 - val_accuracy: 0.6429 - val_loss: 0.6563\n",
      "Epoch 14/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 867us/step - accuracy: 0.6744 - loss: 0.6436 - val_accuracy: 0.6429 - val_loss: 0.6554\n",
      "Epoch 15/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 820us/step - accuracy: 0.6485 - loss: 0.6528 - val_accuracy: 0.6429 - val_loss: 0.6546\n",
      "Epoch 16/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 849us/step - accuracy: 0.6359 - loss: 0.6575 - val_accuracy: 0.6429 - val_loss: 0.6541\n",
      "Epoch 17/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 922us/step - accuracy: 0.6639 - loss: 0.6445 - val_accuracy: 0.6429 - val_loss: 0.6536\n",
      "Epoch 18/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 804us/step - accuracy: 0.6506 - loss: 0.6499 - val_accuracy: 0.6429 - val_loss: 0.6532\n",
      "Epoch 19/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 806us/step - accuracy: 0.6614 - loss: 0.6442 - val_accuracy: 0.6429 - val_loss: 0.6529\n",
      "Epoch 20/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 882us/step - accuracy: 0.6714 - loss: 0.6389 - val_accuracy: 0.6429 - val_loss: 0.6526\n",
      "Epoch 21/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 792us/step - accuracy: 0.6361 - loss: 0.6560 - val_accuracy: 0.6429 - val_loss: 0.6525\n",
      "Epoch 22/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 917us/step - accuracy: 0.6290 - loss: 0.6595 - val_accuracy: 0.6429 - val_loss: 0.6523\n",
      "Epoch 23/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 791us/step - accuracy: 0.6388 - loss: 0.6544 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 24/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 873us/step - accuracy: 0.6325 - loss: 0.6576 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 25/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 735us/step - accuracy: 0.6700 - loss: 0.6373 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 26/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 939us/step - accuracy: 0.6227 - loss: 0.6631 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 27/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6376 - loss: 0.6548 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 28/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 923us/step - accuracy: 0.6502 - loss: 0.6477 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 29/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 963us/step - accuracy: 0.6570 - loss: 0.6436 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 30/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 817us/step - accuracy: 0.6401 - loss: 0.6534 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 31/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 875us/step - accuracy: 0.6850 - loss: 0.6272 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 32/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 805us/step - accuracy: 0.6379 - loss: 0.6547 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 33/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 830us/step - accuracy: 0.6600 - loss: 0.6417 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 34/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 808us/step - accuracy: 0.6380 - loss: 0.6547 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 35/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6513 - loss: 0.6468 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 36/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 904us/step - accuracy: 0.6509 - loss: 0.6470 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 37/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 855us/step - accuracy: 0.6318 - loss: 0.6585 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 38/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6562 - loss: 0.6438 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 39/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 849us/step - accuracy: 0.6797 - loss: 0.6294 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 40/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 831us/step - accuracy: 0.6605 - loss: 0.6411 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 41/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6394 - loss: 0.6539 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 42/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6907 - loss: 0.6226 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 43/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6726 - loss: 0.6336 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 44/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6602 - loss: 0.6412 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 45/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 834us/step - accuracy: 0.6456 - loss: 0.6502 - val_accuracy: 0.6429 - val_loss: 0.6518\n",
      "Epoch 46/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 789us/step - accuracy: 0.6754 - loss: 0.6318 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 47/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6495 - loss: 0.6478 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 48/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 835us/step - accuracy: 0.6612 - loss: 0.6405 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 49/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 945us/step - accuracy: 0.6505 - loss: 0.6472 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 50/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 781us/step - accuracy: 0.6897 - loss: 0.6228 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 51/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 791us/step - accuracy: 0.6446 - loss: 0.6508 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 52/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 802us/step - accuracy: 0.6705 - loss: 0.6347 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 53/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 827us/step - accuracy: 0.6454 - loss: 0.6504 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 54/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 848us/step - accuracy: 0.6265 - loss: 0.6622 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 55/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6541 - loss: 0.6449 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 56/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 812us/step - accuracy: 0.6598 - loss: 0.6413 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 57/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 860us/step - accuracy: 0.6514 - loss: 0.6466 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 58/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 968us/step - accuracy: 0.6383 - loss: 0.6549 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 59/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 893us/step - accuracy: 0.6614 - loss: 0.6403 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 60/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6475 - loss: 0.6491 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 61/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 754us/step - accuracy: 0.6404 - loss: 0.6535 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 62/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 764us/step - accuracy: 0.6870 - loss: 0.6242 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 63/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 871us/step - accuracy: 0.6548 - loss: 0.6444 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 64/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 808us/step - accuracy: 0.6478 - loss: 0.6489 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 65/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 777us/step - accuracy: 0.6613 - loss: 0.6404 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 66/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6467 - loss: 0.6495 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 67/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6789 - loss: 0.6293 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 68/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 832us/step - accuracy: 0.6444 - loss: 0.6510 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 69/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 847us/step - accuracy: 0.6653 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 70/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 796us/step - accuracy: 0.6471 - loss: 0.6493 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 71/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6504 - loss: 0.6472 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 72/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 748us/step - accuracy: 0.6297 - loss: 0.6602 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 73/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 969us/step - accuracy: 0.6420 - loss: 0.6526 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 74/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 727us/step - accuracy: 0.6700 - loss: 0.6349 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 75/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 769us/step - accuracy: 0.6319 - loss: 0.6590 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 76/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 835us/step - accuracy: 0.6358 - loss: 0.6564 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 77/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 837us/step - accuracy: 0.6529 - loss: 0.6456 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 78/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6411 - loss: 0.6531 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 79/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 928us/step - accuracy: 0.6533 - loss: 0.6454 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 80/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 805us/step - accuracy: 0.6702 - loss: 0.6347 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 81/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 913us/step - accuracy: 0.6507 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 82/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 865us/step - accuracy: 0.6413 - loss: 0.6530 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 83/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 749us/step - accuracy: 0.6646 - loss: 0.6383 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 84/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 820us/step - accuracy: 0.6637 - loss: 0.6388 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 85/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 770us/step - accuracy: 0.6650 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 86/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 804us/step - accuracy: 0.6386 - loss: 0.6547 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 87/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 824us/step - accuracy: 0.6561 - loss: 0.6437 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 88/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 782us/step - accuracy: 0.6580 - loss: 0.6424 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 89/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 882us/step - accuracy: 0.6849 - loss: 0.6254 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 90/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 785us/step - accuracy: 0.6620 - loss: 0.6399 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 91/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 878us/step - accuracy: 0.6330 - loss: 0.6583 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 92/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6563 - loss: 0.6435 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 93/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 786us/step - accuracy: 0.6449 - loss: 0.6507 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 94/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 813us/step - accuracy: 0.6249 - loss: 0.6634 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 95/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6542 - loss: 0.6449 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 96/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 862us/step - accuracy: 0.6530 - loss: 0.6456 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 97/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 882us/step - accuracy: 0.6548 - loss: 0.6444 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 98/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 828us/step - accuracy: 0.6237 - loss: 0.6643 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 99/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 782us/step - accuracy: 0.6337 - loss: 0.6578 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 100/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6290 - loss: 0.6608 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 101/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 860us/step - accuracy: 0.6345 - loss: 0.6573 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 102/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6395 - loss: 0.6541 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 103/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 801us/step - accuracy: 0.6694 - loss: 0.6352 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 104/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 824us/step - accuracy: 0.6942 - loss: 0.6195 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 105/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 834us/step - accuracy: 0.6121 - loss: 0.6714 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 106/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 811us/step - accuracy: 0.6681 - loss: 0.6361 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 107/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 916us/step - accuracy: 0.6714 - loss: 0.6339 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 108/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 747us/step - accuracy: 0.6799 - loss: 0.6286 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 109/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6412 - loss: 0.6531 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 110/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 780us/step - accuracy: 0.6520 - loss: 0.6462 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 111/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 801us/step - accuracy: 0.6713 - loss: 0.6340 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 112/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6665 - loss: 0.6370 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 113/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6338 - loss: 0.6578 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 114/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6521 - loss: 0.6462 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 115/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6381 - loss: 0.6550 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 116/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 996us/step - accuracy: 0.6341 - loss: 0.6575 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 117/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.5987 - loss: 0.6799 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 118/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 846us/step - accuracy: 0.6273 - loss: 0.6619 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 119/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6494 - loss: 0.6479 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 120/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 794us/step - accuracy: 0.6665 - loss: 0.6370 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 121/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6562 - loss: 0.6436 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 122/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6831 - loss: 0.6264 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 123/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6515 - loss: 0.6466 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 124/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 896us/step - accuracy: 0.6183 - loss: 0.6677 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 125/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 812us/step - accuracy: 0.6439 - loss: 0.6514 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 126/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 807us/step - accuracy: 0.6188 - loss: 0.6674 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 127/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6519 - loss: 0.6463 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 128/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 811us/step - accuracy: 0.6446 - loss: 0.6509 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 129/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6394 - loss: 0.6543 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 130/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 873us/step - accuracy: 0.6619 - loss: 0.6399 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 131/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 872us/step - accuracy: 0.6342 - loss: 0.6576 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 132/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 889us/step - accuracy: 0.6420 - loss: 0.6526 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 133/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6232 - loss: 0.6646 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 134/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6255 - loss: 0.6631 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 135/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 787us/step - accuracy: 0.6276 - loss: 0.6618 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 136/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 810us/step - accuracy: 0.6239 - loss: 0.6640 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 137/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 811us/step - accuracy: 0.6372 - loss: 0.6556 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 138/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 794us/step - accuracy: 0.6598 - loss: 0.6413 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 139/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6702 - loss: 0.6346 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 140/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 971us/step - accuracy: 0.6742 - loss: 0.6321 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 141/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 892us/step - accuracy: 0.6477 - loss: 0.6490 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 142/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 909us/step - accuracy: 0.6319 - loss: 0.6590 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 143/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6589 - loss: 0.6418 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 144/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6452 - loss: 0.6506 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 145/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 798us/step - accuracy: 0.6374 - loss: 0.6556 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 146/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 871us/step - accuracy: 0.6701 - loss: 0.6346 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 147/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6836 - loss: 0.6262 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 148/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 803us/step - accuracy: 0.6365 - loss: 0.6562 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 149/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6627 - loss: 0.6394 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 150/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 770us/step - accuracy: 0.6428 - loss: 0.6521 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 151/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 846us/step - accuracy: 0.6880 - loss: 0.6234 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 152/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 812us/step - accuracy: 0.6558 - loss: 0.6438 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 153/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6546 - loss: 0.6446 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 154/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 859us/step - accuracy: 0.6676 - loss: 0.6364 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 155/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6348 - loss: 0.6571 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 156/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 736us/step - accuracy: 0.6520 - loss: 0.6462 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 157/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 819us/step - accuracy: 0.6359 - loss: 0.6565 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 158/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 954us/step - accuracy: 0.6534 - loss: 0.6454 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 159/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 787us/step - accuracy: 0.6737 - loss: 0.6325 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 160/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 774us/step - accuracy: 0.6251 - loss: 0.6633 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 161/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 730us/step - accuracy: 0.6649 - loss: 0.6381 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 162/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6820 - loss: 0.6271 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 163/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 798us/step - accuracy: 0.6644 - loss: 0.6383 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 164/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 800us/step - accuracy: 0.6334 - loss: 0.6581 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 165/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 786us/step - accuracy: 0.6713 - loss: 0.6340 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 166/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 860us/step - accuracy: 0.6778 - loss: 0.6298 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 167/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 925us/step - accuracy: 0.6506 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 168/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6404 - loss: 0.6536 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 169/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 798us/step - accuracy: 0.6371 - loss: 0.6557 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 170/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 895us/step - accuracy: 0.6678 - loss: 0.6362 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 171/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 821us/step - accuracy: 0.6725 - loss: 0.6331 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 172/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 827us/step - accuracy: 0.6549 - loss: 0.6444 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 173/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6587 - loss: 0.6420 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 174/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 941us/step - accuracy: 0.6406 - loss: 0.6534 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 175/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6472 - loss: 0.6492 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 176/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 734us/step - accuracy: 0.6755 - loss: 0.6313 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 177/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 994us/step - accuracy: 0.6652 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 178/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 859us/step - accuracy: 0.6691 - loss: 0.6354 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 179/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 757us/step - accuracy: 0.6368 - loss: 0.6559 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 180/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 918us/step - accuracy: 0.6717 - loss: 0.6338 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 181/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 782us/step - accuracy: 0.6095 - loss: 0.6730 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 182/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 766us/step - accuracy: 0.6446 - loss: 0.6509 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 183/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 796us/step - accuracy: 0.6460 - loss: 0.6500 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 184/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 773us/step - accuracy: 0.6473 - loss: 0.6492 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 185/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 824us/step - accuracy: 0.6301 - loss: 0.6601 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 186/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 749us/step - accuracy: 0.6884 - loss: 0.6232 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 187/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 798us/step - accuracy: 0.6350 - loss: 0.6570 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 188/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6648 - loss: 0.6382 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 189/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6602 - loss: 0.6411 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 190/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 791us/step - accuracy: 0.6329 - loss: 0.6583 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 191/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 776us/step - accuracy: 0.6444 - loss: 0.6510 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 192/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 917us/step - accuracy: 0.6472 - loss: 0.6493 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 193/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6547 - loss: 0.6445 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 194/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 788us/step - accuracy: 0.6717 - loss: 0.6338 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 195/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 824us/step - accuracy: 0.6561 - loss: 0.6436 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 196/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 786us/step - accuracy: 0.6411 - loss: 0.6531 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 197/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 912us/step - accuracy: 0.6602 - loss: 0.6410 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 198/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 830us/step - accuracy: 0.6326 - loss: 0.6585 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 199/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 779us/step - accuracy: 0.6457 - loss: 0.6502 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 200/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 909us/step - accuracy: 0.6483 - loss: 0.6486 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 201/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 719us/step - accuracy: 0.6625 - loss: 0.6396 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 202/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 759us/step - accuracy: 0.6723 - loss: 0.6334 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 203/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6769 - loss: 0.6304 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 204/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 770us/step - accuracy: 0.6438 - loss: 0.6514 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 205/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 807us/step - accuracy: 0.6556 - loss: 0.6439 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 206/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6180 - loss: 0.6677 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 207/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6543 - loss: 0.6448 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 208/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 924us/step - accuracy: 0.6703 - loss: 0.6346 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 209/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6535 - loss: 0.6453 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 210/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 879us/step - accuracy: 0.6222 - loss: 0.6651 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 211/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 822us/step - accuracy: 0.6353 - loss: 0.6568 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 212/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6216 - loss: 0.6655 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 213/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 789us/step - accuracy: 0.6423 - loss: 0.6524 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 214/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 983us/step - accuracy: 0.6535 - loss: 0.6453 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 215/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 778us/step - accuracy: 0.6713 - loss: 0.6340 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 216/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 759us/step - accuracy: 0.6675 - loss: 0.6364 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 217/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 834us/step - accuracy: 0.6614 - loss: 0.6402 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 218/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 799us/step - accuracy: 0.6570 - loss: 0.6431 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 219/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 819us/step - accuracy: 0.6783 - loss: 0.6296 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 220/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 877us/step - accuracy: 0.6180 - loss: 0.6678 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 221/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6807 - loss: 0.6280 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 222/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 835us/step - accuracy: 0.6509 - loss: 0.6469 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 223/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 718us/step - accuracy: 0.6619 - loss: 0.6400 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 224/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 762us/step - accuracy: 0.6615 - loss: 0.6402 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 225/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 744us/step - accuracy: 0.6771 - loss: 0.6303 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 226/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 792us/step - accuracy: 0.6763 - loss: 0.6308 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 227/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 819us/step - accuracy: 0.6351 - loss: 0.6569 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 228/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 790us/step - accuracy: 0.6400 - loss: 0.6538 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 229/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6276 - loss: 0.6616 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 230/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 797us/step - accuracy: 0.6540 - loss: 0.6449 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 231/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6203 - loss: 0.6663 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 232/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 853us/step - accuracy: 0.6562 - loss: 0.6436 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 233/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 754us/step - accuracy: 0.6367 - loss: 0.6559 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 234/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 896us/step - accuracy: 0.6272 - loss: 0.6619 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 235/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 883us/step - accuracy: 0.6564 - loss: 0.6435 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 236/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 869us/step - accuracy: 0.6706 - loss: 0.6345 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 237/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6577 - loss: 0.6426 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 238/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1000us/step - accuracy: 0.6645 - loss: 0.6382 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 239/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 809us/step - accuracy: 0.6692 - loss: 0.6353 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 240/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 934us/step - accuracy: 0.6120 - loss: 0.6715 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 241/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 865us/step - accuracy: 0.6573 - loss: 0.6429 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 242/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6398 - loss: 0.6540 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 243/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6586 - loss: 0.6421 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 244/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 804us/step - accuracy: 0.6229 - loss: 0.6646 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 245/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 897us/step - accuracy: 0.6603 - loss: 0.6410 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 246/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6238 - loss: 0.6641 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 247/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 851us/step - accuracy: 0.6382 - loss: 0.6549 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 248/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6303 - loss: 0.6599 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 249/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 787us/step - accuracy: 0.6722 - loss: 0.6335 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 250/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 876us/step - accuracy: 0.6875 - loss: 0.6237 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 251/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6701 - loss: 0.6348 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 252/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 845us/step - accuracy: 0.6652 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 253/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6703 - loss: 0.6347 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 254/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6619 - loss: 0.6399 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 255/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 786us/step - accuracy: 0.6499 - loss: 0.6475 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 256/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 791us/step - accuracy: 0.6500 - loss: 0.6475 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 257/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6396 - loss: 0.6541 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 258/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 795us/step - accuracy: 0.6857 - loss: 0.6249 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 259/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 794us/step - accuracy: 0.6928 - loss: 0.6204 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 260/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 703us/step - accuracy: 0.6538 - loss: 0.6451 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 261/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 831us/step - accuracy: 0.6481 - loss: 0.6487 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 262/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 897us/step - accuracy: 0.6788 - loss: 0.6293 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 263/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 767us/step - accuracy: 0.6473 - loss: 0.6492 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 264/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 783us/step - accuracy: 0.6603 - loss: 0.6410 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 265/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6899 - loss: 0.6223 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 266/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 868us/step - accuracy: 0.6689 - loss: 0.6355 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 267/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.5912 - loss: 0.6849 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 268/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 912us/step - accuracy: 0.6564 - loss: 0.6435 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 269/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 802us/step - accuracy: 0.6446 - loss: 0.6509 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 270/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 804us/step - accuracy: 0.6864 - loss: 0.6244 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 271/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6536 - loss: 0.6452 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 272/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 799us/step - accuracy: 0.6316 - loss: 0.6592 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 273/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 874us/step - accuracy: 0.6611 - loss: 0.6404 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 274/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6455 - loss: 0.6504 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 275/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6440 - loss: 0.6513 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 276/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 795us/step - accuracy: 0.6815 - loss: 0.6275 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 277/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 805us/step - accuracy: 0.6552 - loss: 0.6442 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 278/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 813us/step - accuracy: 0.6566 - loss: 0.6433 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 279/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 900us/step - accuracy: 0.6642 - loss: 0.6385 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 280/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6454 - loss: 0.6505 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 281/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6587 - loss: 0.6420 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 282/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 849us/step - accuracy: 0.6309 - loss: 0.6596 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 283/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6414 - loss: 0.6530 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 284/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6585 - loss: 0.6421 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 285/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 850us/step - accuracy: 0.6179 - loss: 0.6679 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 286/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 831us/step - accuracy: 0.6230 - loss: 0.6646 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 287/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 768us/step - accuracy: 0.6366 - loss: 0.6560 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 288/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 847us/step - accuracy: 0.6610 - loss: 0.6405 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 289/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 967us/step - accuracy: 0.6628 - loss: 0.6393 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 290/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6605 - loss: 0.6408 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 291/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 967us/step - accuracy: 0.6509 - loss: 0.6469 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 292/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 974us/step - accuracy: 0.6792 - loss: 0.6290 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 293/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6477 - loss: 0.6489 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 294/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 876us/step - accuracy: 0.6646 - loss: 0.6382 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 295/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6465 - loss: 0.6497 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 296/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 824us/step - accuracy: 0.6717 - loss: 0.6337 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 297/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 772us/step - accuracy: 0.6615 - loss: 0.6402 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 298/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6814 - loss: 0.6276 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 299/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 806us/step - accuracy: 0.6596 - loss: 0.6414 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 300/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6851 - loss: 0.6252 - val_accuracy: 0.6429 - val_loss: 0.6520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f88348b6c90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:15.660028Z",
     "start_time": "2024-07-27T08:12:15.652965Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "2789053fe7ec02e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m)              │            \u001B[38;5;34m72\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m120\u001B[0m)            │         \u001B[38;5;34m1,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m7,744\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │           \u001B[38;5;34m390\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              │            \u001B[38;5;34m14\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │             \u001B[38;5;34m3\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m27,911\u001B[0m (109.03 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,911</span> (109.03 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m9,303\u001B[0m (36.34 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,303</span> (36.34 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m18,608\u001B[0m (72.69 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,608</span> (72.69 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:36.613955Z",
     "start_time": "2024-07-27T08:12:15.660742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import normalize, scale\n",
    "X = scale(X)\n",
    "\n",
    "model.fit(X, y, epochs=300, batch_size=10, validation_split=0.2, verbose=1)"
   ],
   "id": "19c7cfdd04f2093a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6511 - loss: 0.6468 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 2/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 815us/step - accuracy: 0.6711 - loss: 0.6342 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 3/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 914us/step - accuracy: 0.6614 - loss: 0.6403 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 4/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6646 - loss: 0.6383 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 5/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 899us/step - accuracy: 0.6557 - loss: 0.6439 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 6/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6453 - loss: 0.6505 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 7/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6603 - loss: 0.6410 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 8/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 957us/step - accuracy: 0.6694 - loss: 0.6353 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 9/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 905us/step - accuracy: 0.6515 - loss: 0.6466 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 10/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 837us/step - accuracy: 0.6471 - loss: 0.6493 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 11/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 898us/step - accuracy: 0.6276 - loss: 0.6616 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 12/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 866us/step - accuracy: 0.6506 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 13/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6470 - loss: 0.6494 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 14/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 930us/step - accuracy: 0.6506 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 15/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 871us/step - accuracy: 0.6636 - loss: 0.6389 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 16/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 869us/step - accuracy: 0.6573 - loss: 0.6429 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 17/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6435 - loss: 0.6516 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 18/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 826us/step - accuracy: 0.6371 - loss: 0.6556 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 19/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 981us/step - accuracy: 0.6264 - loss: 0.6624 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 20/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 831us/step - accuracy: 0.6233 - loss: 0.6642 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 21/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 945us/step - accuracy: 0.6539 - loss: 0.6450 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 22/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6346 - loss: 0.6572 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 23/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6514 - loss: 0.6466 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 24/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6579 - loss: 0.6425 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 25/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 877us/step - accuracy: 0.6329 - loss: 0.6582 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 26/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 990us/step - accuracy: 0.6632 - loss: 0.6392 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 27/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6610 - loss: 0.6405 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 28/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6769 - loss: 0.6305 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 29/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 925us/step - accuracy: 0.6505 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 30/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6369 - loss: 0.6557 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 31/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6528 - loss: 0.6457 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 32/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 873us/step - accuracy: 0.6478 - loss: 0.6488 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 33/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 910us/step - accuracy: 0.6308 - loss: 0.6596 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 34/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6676 - loss: 0.6364 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 35/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6594 - loss: 0.6416 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 36/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 922us/step - accuracy: 0.6317 - loss: 0.6590 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 37/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 951us/step - accuracy: 0.6626 - loss: 0.6396 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 38/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 903us/step - accuracy: 0.6712 - loss: 0.6341 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 39/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 967us/step - accuracy: 0.6659 - loss: 0.6374 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 40/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 877us/step - accuracy: 0.6822 - loss: 0.6271 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 41/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 876us/step - accuracy: 0.6296 - loss: 0.6605 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 42/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 965us/step - accuracy: 0.6289 - loss: 0.6608 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 43/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 855us/step - accuracy: 0.6494 - loss: 0.6479 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 44/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 882us/step - accuracy: 0.6522 - loss: 0.6461 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 45/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 928us/step - accuracy: 0.6546 - loss: 0.6446 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 46/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 931us/step - accuracy: 0.6372 - loss: 0.6557 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 47/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 867us/step - accuracy: 0.6862 - loss: 0.6246 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 48/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6652 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 49/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6579 - loss: 0.6425 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 50/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6434 - loss: 0.6517 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 51/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6327 - loss: 0.6585 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 52/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6359 - loss: 0.6564 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 53/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 860us/step - accuracy: 0.6098 - loss: 0.6729 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 54/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 916us/step - accuracy: 0.6647 - loss: 0.6382 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 55/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6510 - loss: 0.6468 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 56/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 869us/step - accuracy: 0.6252 - loss: 0.6633 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 57/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6674 - loss: 0.6364 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 58/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 936us/step - accuracy: 0.6430 - loss: 0.6519 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 59/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 954us/step - accuracy: 0.6472 - loss: 0.6493 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 60/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 832us/step - accuracy: 0.6618 - loss: 0.6400 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 61/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 956us/step - accuracy: 0.6479 - loss: 0.6489 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 62/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 859us/step - accuracy: 0.6503 - loss: 0.6473 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 63/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 945us/step - accuracy: 0.6311 - loss: 0.6596 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 64/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 904us/step - accuracy: 0.6762 - loss: 0.6309 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 65/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6669 - loss: 0.6368 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 66/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 888us/step - accuracy: 0.6676 - loss: 0.6363 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 67/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6799 - loss: 0.6286 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 68/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6486 - loss: 0.6484 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 69/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6393 - loss: 0.6543 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 70/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 990us/step - accuracy: 0.6658 - loss: 0.6375 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 71/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 812us/step - accuracy: 0.6362 - loss: 0.6562 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 72/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6579 - loss: 0.6425 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 73/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 862us/step - accuracy: 0.6573 - loss: 0.6429 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 74/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 887us/step - accuracy: 0.6141 - loss: 0.6701 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 75/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 850us/step - accuracy: 0.6517 - loss: 0.6464 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 76/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 835us/step - accuracy: 0.6572 - loss: 0.6429 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 77/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 867us/step - accuracy: 0.6833 - loss: 0.6264 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 78/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 860us/step - accuracy: 0.6592 - loss: 0.6417 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 79/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6713 - loss: 0.6341 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 80/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 904us/step - accuracy: 0.6496 - loss: 0.6478 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 81/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 809us/step - accuracy: 0.6466 - loss: 0.6496 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 82/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 874us/step - accuracy: 0.6633 - loss: 0.6391 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 83/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6564 - loss: 0.6435 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 84/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 853us/step - accuracy: 0.6361 - loss: 0.6563 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 85/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 873us/step - accuracy: 0.6271 - loss: 0.6620 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 86/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 897us/step - accuracy: 0.6597 - loss: 0.6413 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 87/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 881us/step - accuracy: 0.6528 - loss: 0.6457 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 88/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 982us/step - accuracy: 0.6229 - loss: 0.6647 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 89/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 891us/step - accuracy: 0.6392 - loss: 0.6543 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 90/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 881us/step - accuracy: 0.6685 - loss: 0.6357 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 91/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 806us/step - accuracy: 0.6404 - loss: 0.6536 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 92/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 828us/step - accuracy: 0.6656 - loss: 0.6376 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 93/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 963us/step - accuracy: 0.6493 - loss: 0.6479 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 94/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6642 - loss: 0.6385 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 95/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6588 - loss: 0.6419 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 96/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 908us/step - accuracy: 0.6546 - loss: 0.6445 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 97/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6523 - loss: 0.6460 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 98/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 835us/step - accuracy: 0.6473 - loss: 0.6492 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 99/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 996us/step - accuracy: 0.6324 - loss: 0.6586 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 100/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 872us/step - accuracy: 0.6678 - loss: 0.6362 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 101/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 810us/step - accuracy: 0.6563 - loss: 0.6435 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 102/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 949us/step - accuracy: 0.6506 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 103/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 990us/step - accuracy: 0.6391 - loss: 0.6544 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 104/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 810us/step - accuracy: 0.6530 - loss: 0.6456 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 105/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 926us/step - accuracy: 0.6145 - loss: 0.6699 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 106/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6815 - loss: 0.6277 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 107/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6495 - loss: 0.6478 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 108/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 820us/step - accuracy: 0.6250 - loss: 0.6632 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 109/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6565 - loss: 0.6434 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 110/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 897us/step - accuracy: 0.6840 - loss: 0.6259 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 111/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 899us/step - accuracy: 0.6434 - loss: 0.6517 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 112/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6398 - loss: 0.6539 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 113/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 847us/step - accuracy: 0.6595 - loss: 0.6415 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 114/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 884us/step - accuracy: 0.6722 - loss: 0.6335 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 115/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6414 - loss: 0.6529 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 116/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6401 - loss: 0.6538 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 117/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6667 - loss: 0.6369 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 118/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 865us/step - accuracy: 0.6644 - loss: 0.6384 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 119/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 885us/step - accuracy: 0.6669 - loss: 0.6367 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 120/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 858us/step - accuracy: 0.6763 - loss: 0.6308 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 121/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 846us/step - accuracy: 0.6592 - loss: 0.6417 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 122/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 983us/step - accuracy: 0.6632 - loss: 0.6391 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 123/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 937us/step - accuracy: 0.6377 - loss: 0.6553 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 124/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6621 - loss: 0.6399 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 125/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 830us/step - accuracy: 0.6625 - loss: 0.6395 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 126/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 819us/step - accuracy: 0.6837 - loss: 0.6261 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 127/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 867us/step - accuracy: 0.6495 - loss: 0.6478 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 128/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 907us/step - accuracy: 0.6577 - loss: 0.6427 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 129/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 850us/step - accuracy: 0.6713 - loss: 0.6340 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 130/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 906us/step - accuracy: 0.6563 - loss: 0.6435 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 131/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 795us/step - accuracy: 0.6320 - loss: 0.6589 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 132/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6675 - loss: 0.6364 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 133/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 967us/step - accuracy: 0.6461 - loss: 0.6500 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 134/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6749 - loss: 0.6317 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 135/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 797us/step - accuracy: 0.6467 - loss: 0.6496 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 136/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 817us/step - accuracy: 0.6422 - loss: 0.6524 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 137/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6816 - loss: 0.6275 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 138/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 865us/step - accuracy: 0.6637 - loss: 0.6388 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 139/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 970us/step - accuracy: 0.6545 - loss: 0.6447 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 140/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 875us/step - accuracy: 0.6473 - loss: 0.6492 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 141/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 921us/step - accuracy: 0.6957 - loss: 0.6185 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 142/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 877us/step - accuracy: 0.6381 - loss: 0.6551 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 143/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6296 - loss: 0.6604 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 144/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 873us/step - accuracy: 0.6543 - loss: 0.6448 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 145/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 874us/step - accuracy: 0.6317 - loss: 0.6591 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 146/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6465 - loss: 0.6497 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 147/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 809us/step - accuracy: 0.6814 - loss: 0.6275 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 148/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6648 - loss: 0.6380 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 149/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6655 - loss: 0.6377 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 150/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6938 - loss: 0.6197 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 151/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 980us/step - accuracy: 0.6362 - loss: 0.6562 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 152/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6379 - loss: 0.6552 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 153/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6285 - loss: 0.6611 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 154/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6589 - loss: 0.6419 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 155/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 832us/step - accuracy: 0.6640 - loss: 0.6386 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 156/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 849us/step - accuracy: 0.6776 - loss: 0.6299 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 157/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6420 - loss: 0.6526 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 158/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 997us/step - accuracy: 0.6814 - loss: 0.6276 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 159/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 989us/step - accuracy: 0.6450 - loss: 0.6507 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 160/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6623 - loss: 0.6397 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 161/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 906us/step - accuracy: 0.6539 - loss: 0.6451 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 162/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6669 - loss: 0.6368 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 163/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6172 - loss: 0.6684 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 164/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 850us/step - accuracy: 0.6453 - loss: 0.6505 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 165/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6299 - loss: 0.6603 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 166/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 961us/step - accuracy: 0.6163 - loss: 0.6689 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 167/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6740 - loss: 0.6322 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 168/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 939us/step - accuracy: 0.6615 - loss: 0.6401 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 169/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 823us/step - accuracy: 0.6537 - loss: 0.6452 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 170/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 959us/step - accuracy: 0.6519 - loss: 0.6463 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 171/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 853us/step - accuracy: 0.6424 - loss: 0.6523 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 172/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 987us/step - accuracy: 0.6328 - loss: 0.6584 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 173/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 886us/step - accuracy: 0.6529 - loss: 0.6457 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 174/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 894us/step - accuracy: 0.6557 - loss: 0.6440 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 175/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 848us/step - accuracy: 0.6931 - loss: 0.6200 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 176/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 953us/step - accuracy: 0.6464 - loss: 0.6498 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 177/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 832us/step - accuracy: 0.6421 - loss: 0.6525 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 178/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 868us/step - accuracy: 0.6489 - loss: 0.6482 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 179/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 814us/step - accuracy: 0.6333 - loss: 0.6581 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 180/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 824us/step - accuracy: 0.6516 - loss: 0.6465 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 181/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 871us/step - accuracy: 0.6665 - loss: 0.6370 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 182/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 932us/step - accuracy: 0.6400 - loss: 0.6539 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 183/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6446 - loss: 0.6510 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 184/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6508 - loss: 0.6470 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 185/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 879us/step - accuracy: 0.6316 - loss: 0.6592 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 186/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 884us/step - accuracy: 0.6546 - loss: 0.6446 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 187/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 897us/step - accuracy: 0.6635 - loss: 0.6389 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 188/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6376 - loss: 0.6555 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 189/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 875us/step - accuracy: 0.6382 - loss: 0.6550 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 190/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 906us/step - accuracy: 0.6618 - loss: 0.6400 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 191/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 889us/step - accuracy: 0.6454 - loss: 0.6504 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 192/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 887us/step - accuracy: 0.6633 - loss: 0.6391 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 193/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 960us/step - accuracy: 0.6506 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 194/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6506 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 195/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 996us/step - accuracy: 0.6843 - loss: 0.6257 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 196/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6956 - loss: 0.6185 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 197/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 923us/step - accuracy: 0.6660 - loss: 0.6373 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 198/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 901us/step - accuracy: 0.6430 - loss: 0.6519 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 199/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6812 - loss: 0.6277 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 200/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6513 - loss: 0.6467 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 201/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 965us/step - accuracy: 0.6770 - loss: 0.6304 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 202/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 981us/step - accuracy: 0.6715 - loss: 0.6338 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 203/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 934us/step - accuracy: 0.6674 - loss: 0.6365 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 204/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 886us/step - accuracy: 0.6430 - loss: 0.6519 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 205/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 956us/step - accuracy: 0.6653 - loss: 0.6378 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 206/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6345 - loss: 0.6572 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 207/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6494 - loss: 0.6478 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 208/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 889us/step - accuracy: 0.6755 - loss: 0.6314 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 209/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 921us/step - accuracy: 0.6418 - loss: 0.6526 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 210/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 862us/step - accuracy: 0.6327 - loss: 0.6584 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 211/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6690 - loss: 0.6355 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 212/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 849us/step - accuracy: 0.6512 - loss: 0.6467 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 213/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 882us/step - accuracy: 0.6279 - loss: 0.6615 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 214/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6538 - loss: 0.6451 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 215/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6385 - loss: 0.6547 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 216/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 926us/step - accuracy: 0.7001 - loss: 0.6159 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 217/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 990us/step - accuracy: 0.6544 - loss: 0.6447 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 218/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6436 - loss: 0.6515 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 219/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 872us/step - accuracy: 0.6701 - loss: 0.6348 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 220/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6525 - loss: 0.6459 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 221/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 884us/step - accuracy: 0.6471 - loss: 0.6493 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 222/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 878us/step - accuracy: 0.6723 - loss: 0.6334 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 223/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 889us/step - accuracy: 0.6594 - loss: 0.6415 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 224/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 860us/step - accuracy: 0.6341 - loss: 0.6576 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 225/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 908us/step - accuracy: 0.6500 - loss: 0.6475 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 226/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 823us/step - accuracy: 0.6387 - loss: 0.6547 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 227/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6439 - loss: 0.6514 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 228/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 909us/step - accuracy: 0.6115 - loss: 0.6720 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 229/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 925us/step - accuracy: 0.6449 - loss: 0.6507 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 230/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 947us/step - accuracy: 0.6361 - loss: 0.6564 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 231/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 883us/step - accuracy: 0.6442 - loss: 0.6512 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 232/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 874us/step - accuracy: 0.6246 - loss: 0.6637 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 233/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 888us/step - accuracy: 0.6531 - loss: 0.6455 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 234/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 822us/step - accuracy: 0.6226 - loss: 0.6650 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 235/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 922us/step - accuracy: 0.6536 - loss: 0.6452 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 236/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6565 - loss: 0.6434 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 237/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 865us/step - accuracy: 0.6543 - loss: 0.6448 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 238/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 935us/step - accuracy: 0.6634 - loss: 0.6390 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 239/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6752 - loss: 0.6314 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 240/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 843us/step - accuracy: 0.6294 - loss: 0.6607 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 241/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 882us/step - accuracy: 0.6614 - loss: 0.6403 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 242/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 914us/step - accuracy: 0.6443 - loss: 0.6511 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 243/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6617 - loss: 0.6401 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 244/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 846us/step - accuracy: 0.6472 - loss: 0.6493 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 245/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 947us/step - accuracy: 0.6386 - loss: 0.6548 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 246/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 957us/step - accuracy: 0.6772 - loss: 0.6303 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 247/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6463 - loss: 0.6499 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 248/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 902us/step - accuracy: 0.6563 - loss: 0.6435 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 249/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 921us/step - accuracy: 0.6311 - loss: 0.6595 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 250/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 883us/step - accuracy: 0.6526 - loss: 0.6459 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 251/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 887us/step - accuracy: 0.6583 - loss: 0.6422 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 252/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 791us/step - accuracy: 0.6053 - loss: 0.6756 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 253/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 910us/step - accuracy: 0.6873 - loss: 0.6241 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 254/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 927us/step - accuracy: 0.6615 - loss: 0.6402 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 255/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 828us/step - accuracy: 0.6533 - loss: 0.6454 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 256/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 868us/step - accuracy: 0.6203 - loss: 0.6662 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 257/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 821us/step - accuracy: 0.6977 - loss: 0.6175 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 258/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 981us/step - accuracy: 0.6409 - loss: 0.6532 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 259/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6316 - loss: 0.6590 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 260/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 849us/step - accuracy: 0.6549 - loss: 0.6444 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 261/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6318 - loss: 0.6590 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 262/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 885us/step - accuracy: 0.6707 - loss: 0.6344 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 263/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 846us/step - accuracy: 0.6617 - loss: 0.6401 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 264/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 900us/step - accuracy: 0.6845 - loss: 0.6258 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 265/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 969us/step - accuracy: 0.6841 - loss: 0.6259 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 266/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 923us/step - accuracy: 0.6401 - loss: 0.6537 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 267/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 905us/step - accuracy: 0.6578 - loss: 0.6426 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 268/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 813us/step - accuracy: 0.6062 - loss: 0.6751 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 269/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 813us/step - accuracy: 0.6670 - loss: 0.6368 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 270/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 968us/step - accuracy: 0.6730 - loss: 0.6329 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 271/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 835us/step - accuracy: 0.6699 - loss: 0.6349 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 272/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 865us/step - accuracy: 0.6685 - loss: 0.6358 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 273/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 851us/step - accuracy: 0.6483 - loss: 0.6486 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 274/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 869us/step - accuracy: 0.6435 - loss: 0.6516 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 275/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 903us/step - accuracy: 0.6587 - loss: 0.6420 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 276/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 971us/step - accuracy: 0.6537 - loss: 0.6452 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 277/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 988us/step - accuracy: 0.6568 - loss: 0.6432 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 278/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 901us/step - accuracy: 0.6653 - loss: 0.6378 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 279/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6466 - loss: 0.6497 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 280/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 906us/step - accuracy: 0.6589 - loss: 0.6419 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 281/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 984us/step - accuracy: 0.6469 - loss: 0.6495 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 282/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6873 - loss: 0.6239 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 283/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 886us/step - accuracy: 0.6644 - loss: 0.6383 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 284/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 963us/step - accuracy: 0.6630 - loss: 0.6393 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 285/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 810us/step - accuracy: 0.6435 - loss: 0.6516 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 286/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 858us/step - accuracy: 0.6618 - loss: 0.6401 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 287/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 974us/step - accuracy: 0.6398 - loss: 0.6539 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 288/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 843us/step - accuracy: 0.6513 - loss: 0.6467 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 289/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 964us/step - accuracy: 0.6481 - loss: 0.6487 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 290/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 859us/step - accuracy: 0.6363 - loss: 0.6562 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 291/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 902us/step - accuracy: 0.6695 - loss: 0.6352 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 292/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 851us/step - accuracy: 0.6632 - loss: 0.6392 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 293/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 809us/step - accuracy: 0.6709 - loss: 0.6342 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 294/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 904us/step - accuracy: 0.6408 - loss: 0.6533 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 295/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 928us/step - accuracy: 0.6750 - loss: 0.6317 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 296/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6675 - loss: 0.6364 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 297/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 903us/step - accuracy: 0.6601 - loss: 0.6411 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 298/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6566 - loss: 0.6433 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 299/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 896us/step - accuracy: 0.6339 - loss: 0.6577 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 300/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 821us/step - accuracy: 0.6660 - loss: 0.6374 - val_accuracy: 0.6429 - val_loss: 0.6520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f8834872cd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:36.621544Z",
     "start_time": "2024-07-27T08:12:36.614579Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "b97b37a5101cca61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m)              │            \u001B[38;5;34m72\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m120\u001B[0m)            │         \u001B[38;5;34m1,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m7,744\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │           \u001B[38;5;34m390\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              │            \u001B[38;5;34m14\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │             \u001B[38;5;34m3\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m27,911\u001B[0m (109.03 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,911</span> (109.03 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m9,303\u001B[0m (36.34 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,303</span> (36.34 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m18,608\u001B[0m (72.69 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,608</span> (72.69 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:56.942712Z",
     "start_time": "2024-07-27T08:12:36.622046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = normalize(X)\n",
    "\n",
    "history = model.fit(X, y, epochs=300, batch_size=10, validation_split=0.2, verbose=1)"
   ],
   "id": "d0c79bb8b586ea1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6522 - loss: 0.6461 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 2/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 926us/step - accuracy: 0.6634 - loss: 0.6390 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 3/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6581 - loss: 0.6424 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 4/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6268 - loss: 0.6621 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 5/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6805 - loss: 0.6283 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 6/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6671 - loss: 0.6366 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 7/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 891us/step - accuracy: 0.6510 - loss: 0.6469 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 8/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 826us/step - accuracy: 0.6510 - loss: 0.6468 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 9/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6780 - loss: 0.6298 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 10/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6628 - loss: 0.6394 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 11/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 813us/step - accuracy: 0.6682 - loss: 0.6359 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 12/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 834us/step - accuracy: 0.6577 - loss: 0.6426 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 13/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6545 - loss: 0.6447 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 14/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 953us/step - accuracy: 0.6661 - loss: 0.6373 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 15/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6417 - loss: 0.6527 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 16/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 898us/step - accuracy: 0.6374 - loss: 0.6555 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 17/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 822us/step - accuracy: 0.6591 - loss: 0.6417 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 18/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6406 - loss: 0.6534 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 19/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 876us/step - accuracy: 0.6536 - loss: 0.6452 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 20/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6634 - loss: 0.6391 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 21/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 893us/step - accuracy: 0.6454 - loss: 0.6504 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 22/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 940us/step - accuracy: 0.6302 - loss: 0.6601 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 23/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 979us/step - accuracy: 0.6503 - loss: 0.6473 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 24/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 817us/step - accuracy: 0.6216 - loss: 0.6655 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 25/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 880us/step - accuracy: 0.6421 - loss: 0.6525 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 26/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 976us/step - accuracy: 0.6564 - loss: 0.6434 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 27/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6549 - loss: 0.6444 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 28/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 875us/step - accuracy: 0.6558 - loss: 0.6438 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 29/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 964us/step - accuracy: 0.6384 - loss: 0.6548 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 30/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 962us/step - accuracy: 0.6259 - loss: 0.6627 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 31/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 883us/step - accuracy: 0.6562 - loss: 0.6436 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 32/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6301 - loss: 0.6601 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 33/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 882us/step - accuracy: 0.6536 - loss: 0.6452 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 34/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6355 - loss: 0.6567 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 35/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 826us/step - accuracy: 0.6817 - loss: 0.6273 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 36/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 839us/step - accuracy: 0.6516 - loss: 0.6465 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 37/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 859us/step - accuracy: 0.6598 - loss: 0.6413 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 38/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6846 - loss: 0.6254 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 39/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6798 - loss: 0.6285 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 40/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6581 - loss: 0.6423 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 41/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 827us/step - accuracy: 0.6501 - loss: 0.6474 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 42/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6662 - loss: 0.6372 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 43/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 846us/step - accuracy: 0.6606 - loss: 0.6407 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 44/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 848us/step - accuracy: 0.6640 - loss: 0.6386 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 45/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 925us/step - accuracy: 0.6292 - loss: 0.6607 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 46/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 814us/step - accuracy: 0.6609 - loss: 0.6406 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 47/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 930us/step - accuracy: 0.6666 - loss: 0.6370 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 48/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6527 - loss: 0.6457 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 49/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6170 - loss: 0.6683 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 50/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 932us/step - accuracy: 0.6354 - loss: 0.6567 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 51/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 905us/step - accuracy: 0.6565 - loss: 0.6434 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 52/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 884us/step - accuracy: 0.6577 - loss: 0.6426 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 53/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 847us/step - accuracy: 0.6447 - loss: 0.6508 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 54/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 846us/step - accuracy: 0.6821 - loss: 0.6272 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 55/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 878us/step - accuracy: 0.6672 - loss: 0.6366 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 56/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 908us/step - accuracy: 0.6609 - loss: 0.6406 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 57/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 850us/step - accuracy: 0.6410 - loss: 0.6531 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 58/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6362 - loss: 0.6561 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 59/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 805us/step - accuracy: 0.6733 - loss: 0.6328 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 60/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 858us/step - accuracy: 0.6665 - loss: 0.6371 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 61/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 837us/step - accuracy: 0.6569 - loss: 0.6431 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 62/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 860us/step - accuracy: 0.6473 - loss: 0.6492 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 63/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 827us/step - accuracy: 0.6462 - loss: 0.6499 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 64/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 812us/step - accuracy: 0.6420 - loss: 0.6525 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 65/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 874us/step - accuracy: 0.6652 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 66/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 809us/step - accuracy: 0.6605 - loss: 0.6409 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 67/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 810us/step - accuracy: 0.6703 - loss: 0.6347 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 68/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6727 - loss: 0.6331 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 69/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 984us/step - accuracy: 0.6527 - loss: 0.6458 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 70/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 901us/step - accuracy: 0.6700 - loss: 0.6349 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 71/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 880us/step - accuracy: 0.6654 - loss: 0.6378 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 72/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 891us/step - accuracy: 0.6584 - loss: 0.6422 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 73/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 827us/step - accuracy: 0.6806 - loss: 0.6282 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 74/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 889us/step - accuracy: 0.6572 - loss: 0.6430 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 75/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 871us/step - accuracy: 0.6475 - loss: 0.6491 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 76/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 890us/step - accuracy: 0.6386 - loss: 0.6547 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 77/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 827us/step - accuracy: 0.6664 - loss: 0.6372 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 78/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 858us/step - accuracy: 0.6414 - loss: 0.6529 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 79/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 899us/step - accuracy: 0.6491 - loss: 0.6481 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 80/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 814us/step - accuracy: 0.6639 - loss: 0.6387 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 81/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 826us/step - accuracy: 0.6489 - loss: 0.6482 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 82/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6388 - loss: 0.6546 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 83/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 953us/step - accuracy: 0.6399 - loss: 0.6539 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 84/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 823us/step - accuracy: 0.6649 - loss: 0.6381 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 85/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6228 - loss: 0.6648 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 86/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6481 - loss: 0.6487 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 87/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6505 - loss: 0.6472 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 88/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6270 - loss: 0.6620 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 89/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 861us/step - accuracy: 0.6874 - loss: 0.6240 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 90/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6625 - loss: 0.6396 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 91/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 858us/step - accuracy: 0.6433 - loss: 0.6518 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 92/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 958us/step - accuracy: 0.6607 - loss: 0.6407 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 93/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6620 - loss: 0.6399 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 94/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6797 - loss: 0.6287 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 95/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 958us/step - accuracy: 0.6679 - loss: 0.6362 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 96/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6340 - loss: 0.6575 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 97/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 781us/step - accuracy: 0.6765 - loss: 0.6308 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 98/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 811us/step - accuracy: 0.6774 - loss: 0.6302 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 99/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 916us/step - accuracy: 0.6434 - loss: 0.6517 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 100/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 817us/step - accuracy: 0.6670 - loss: 0.6368 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 101/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 823us/step - accuracy: 0.6557 - loss: 0.6439 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 102/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6407 - loss: 0.6533 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 103/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 901us/step - accuracy: 0.6217 - loss: 0.6652 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 104/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 774us/step - accuracy: 0.6149 - loss: 0.6697 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 105/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 848us/step - accuracy: 0.6289 - loss: 0.6608 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 106/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 819us/step - accuracy: 0.6557 - loss: 0.6438 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 107/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6395 - loss: 0.6541 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 108/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6245 - loss: 0.6636 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 109/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6548 - loss: 0.6444 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 110/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 811us/step - accuracy: 0.6215 - loss: 0.6656 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 111/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6903 - loss: 0.6219 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 112/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 777us/step - accuracy: 0.6517 - loss: 0.6465 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 113/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 839us/step - accuracy: 0.6499 - loss: 0.6476 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 114/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 843us/step - accuracy: 0.6423 - loss: 0.6523 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 115/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6331 - loss: 0.6582 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 116/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 867us/step - accuracy: 0.6501 - loss: 0.6475 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 117/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 872us/step - accuracy: 0.6284 - loss: 0.6612 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 118/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6305 - loss: 0.6598 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 119/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 927us/step - accuracy: 0.6248 - loss: 0.6634 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 120/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 912us/step - accuracy: 0.6285 - loss: 0.6611 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 121/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 883us/step - accuracy: 0.6362 - loss: 0.6562 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 122/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 845us/step - accuracy: 0.6476 - loss: 0.6490 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 123/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 880us/step - accuracy: 0.6652 - loss: 0.6378 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 124/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 925us/step - accuracy: 0.6683 - loss: 0.6359 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 125/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 954us/step - accuracy: 0.6310 - loss: 0.6596 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 126/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 922us/step - accuracy: 0.6525 - loss: 0.6459 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 127/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 829us/step - accuracy: 0.6651 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 128/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 832us/step - accuracy: 0.6438 - loss: 0.6515 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 129/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 819us/step - accuracy: 0.6601 - loss: 0.6411 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 130/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 862us/step - accuracy: 0.6656 - loss: 0.6376 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 131/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 970us/step - accuracy: 0.6713 - loss: 0.6340 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 132/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 966us/step - accuracy: 0.6428 - loss: 0.6521 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 133/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 775us/step - accuracy: 0.6665 - loss: 0.6370 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 134/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6728 - loss: 0.6329 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 135/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 872us/step - accuracy: 0.6330 - loss: 0.6583 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 136/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6659 - loss: 0.6374 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 137/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6331 - loss: 0.6582 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 138/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 937us/step - accuracy: 0.6411 - loss: 0.6531 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 139/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 845us/step - accuracy: 0.6800 - loss: 0.6285 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 140/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 934us/step - accuracy: 0.6619 - loss: 0.6400 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 141/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 828us/step - accuracy: 0.6752 - loss: 0.6316 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 142/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 859us/step - accuracy: 0.6536 - loss: 0.6452 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 143/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 918us/step - accuracy: 0.6900 - loss: 0.6221 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 144/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 817us/step - accuracy: 0.6782 - loss: 0.6296 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 145/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 814us/step - accuracy: 0.6544 - loss: 0.6447 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 146/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6544 - loss: 0.6447 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 147/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 874us/step - accuracy: 0.6509 - loss: 0.6469 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 148/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 835us/step - accuracy: 0.6350 - loss: 0.6570 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 149/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 830us/step - accuracy: 0.6237 - loss: 0.6641 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 150/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 855us/step - accuracy: 0.6262 - loss: 0.6626 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 151/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 788us/step - accuracy: 0.6508 - loss: 0.6470 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 152/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6867 - loss: 0.6243 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 153/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 990us/step - accuracy: 0.6643 - loss: 0.6384 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 154/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 975us/step - accuracy: 0.6326 - loss: 0.6585 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 155/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 942us/step - accuracy: 0.6424 - loss: 0.6523 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 156/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 872us/step - accuracy: 0.6246 - loss: 0.6636 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 157/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6629 - loss: 0.6393 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 158/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 853us/step - accuracy: 0.6386 - loss: 0.6548 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 159/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 826us/step - accuracy: 0.6414 - loss: 0.6529 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 160/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 821us/step - accuracy: 0.6463 - loss: 0.6498 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 161/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 839us/step - accuracy: 0.6382 - loss: 0.6550 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 162/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 939us/step - accuracy: 0.6550 - loss: 0.6444 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 163/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 794us/step - accuracy: 0.6456 - loss: 0.6503 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 164/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 908us/step - accuracy: 0.6485 - loss: 0.6485 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 165/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 979us/step - accuracy: 0.6468 - loss: 0.6495 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 166/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 878us/step - accuracy: 0.6584 - loss: 0.6422 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 167/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 778us/step - accuracy: 0.6361 - loss: 0.6563 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 168/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 862us/step - accuracy: 0.6346 - loss: 0.6572 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 169/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 831us/step - accuracy: 0.6242 - loss: 0.6638 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 170/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 793us/step - accuracy: 0.6431 - loss: 0.6519 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 171/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6494 - loss: 0.6479 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 172/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 865us/step - accuracy: 0.6567 - loss: 0.6432 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 173/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6617 - loss: 0.6401 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 174/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6613 - loss: 0.6404 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 175/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.7029 - loss: 0.6140 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 176/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 843us/step - accuracy: 0.6370 - loss: 0.6558 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 177/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6165 - loss: 0.6687 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 178/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6464 - loss: 0.6498 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 179/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 740us/step - accuracy: 0.6343 - loss: 0.6574 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 180/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6591 - loss: 0.6417 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 181/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 804us/step - accuracy: 0.6867 - loss: 0.6243 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 182/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 819us/step - accuracy: 0.6565 - loss: 0.6434 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 183/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6723 - loss: 0.6334 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 184/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 825us/step - accuracy: 0.6437 - loss: 0.6514 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 185/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 810us/step - accuracy: 0.6519 - loss: 0.6463 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 186/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 906us/step - accuracy: 0.6716 - loss: 0.6339 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 187/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 869us/step - accuracy: 0.6967 - loss: 0.6180 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 188/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 890us/step - accuracy: 0.6837 - loss: 0.6261 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 189/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 830us/step - accuracy: 0.6730 - loss: 0.6330 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 190/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6261 - loss: 0.6627 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 191/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6368 - loss: 0.6558 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 192/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 967us/step - accuracy: 0.6491 - loss: 0.6481 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 193/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 909us/step - accuracy: 0.6548 - loss: 0.6445 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 194/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 936us/step - accuracy: 0.6276 - loss: 0.6617 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 195/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 743us/step - accuracy: 0.6632 - loss: 0.6391 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 196/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 852us/step - accuracy: 0.6328 - loss: 0.6583 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 197/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 866us/step - accuracy: 0.6720 - loss: 0.6336 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 198/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 856us/step - accuracy: 0.6188 - loss: 0.6673 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 199/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 881us/step - accuracy: 0.6215 - loss: 0.6655 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 200/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 851us/step - accuracy: 0.6593 - loss: 0.6416 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 201/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 876us/step - accuracy: 0.6703 - loss: 0.6346 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 202/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 911us/step - accuracy: 0.6450 - loss: 0.6507 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 203/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 804us/step - accuracy: 0.6552 - loss: 0.6442 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 204/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 862us/step - accuracy: 0.6552 - loss: 0.6442 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 205/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 802us/step - accuracy: 0.6356 - loss: 0.6567 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 206/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 843us/step - accuracy: 0.6648 - loss: 0.6381 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 207/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6524 - loss: 0.6459 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 208/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 911us/step - accuracy: 0.6577 - loss: 0.6426 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 209/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 890us/step - accuracy: 0.6228 - loss: 0.6647 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 210/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 839us/step - accuracy: 0.6720 - loss: 0.6336 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 211/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6290 - loss: 0.6608 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 212/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 993us/step - accuracy: 0.6306 - loss: 0.6597 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 213/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 863us/step - accuracy: 0.6344 - loss: 0.6574 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 214/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 972us/step - accuracy: 0.6391 - loss: 0.6544 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 215/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 828us/step - accuracy: 0.6341 - loss: 0.6575 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 216/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6592 - loss: 0.6417 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 217/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 868us/step - accuracy: 0.6564 - loss: 0.6434 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 218/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6703 - loss: 0.6346 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 219/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 873us/step - accuracy: 0.6397 - loss: 0.6541 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 220/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 820us/step - accuracy: 0.6365 - loss: 0.6560 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 221/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 862us/step - accuracy: 0.6534 - loss: 0.6454 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 222/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 874us/step - accuracy: 0.6509 - loss: 0.6469 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 223/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 918us/step - accuracy: 0.6431 - loss: 0.6519 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 224/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 979us/step - accuracy: 0.6293 - loss: 0.6606 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 225/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 859us/step - accuracy: 0.6415 - loss: 0.6528 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 226/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 854us/step - accuracy: 0.6834 - loss: 0.6263 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 227/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 824us/step - accuracy: 0.6733 - loss: 0.6327 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 228/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 871us/step - accuracy: 0.6140 - loss: 0.6702 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 229/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 822us/step - accuracy: 0.6521 - loss: 0.6462 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 230/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 934us/step - accuracy: 0.6465 - loss: 0.6497 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 231/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6599 - loss: 0.6412 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 232/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 870us/step - accuracy: 0.6651 - loss: 0.6379 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 233/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 851us/step - accuracy: 0.6398 - loss: 0.6539 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 234/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 845us/step - accuracy: 0.6573 - loss: 0.6429 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 235/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 866us/step - accuracy: 0.6620 - loss: 0.6399 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 236/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 847us/step - accuracy: 0.6506 - loss: 0.6471 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 237/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 929us/step - accuracy: 0.6545 - loss: 0.6447 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 238/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6538 - loss: 0.6451 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 239/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 988us/step - accuracy: 0.6658 - loss: 0.6375 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 240/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 782us/step - accuracy: 0.6449 - loss: 0.6507 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 241/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 880us/step - accuracy: 0.6284 - loss: 0.6612 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 242/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 866us/step - accuracy: 0.6977 - loss: 0.6171 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 243/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 822us/step - accuracy: 0.6454 - loss: 0.6505 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 244/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6684 - loss: 0.6359 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 245/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 864us/step - accuracy: 0.6600 - loss: 0.6412 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 246/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.6740 - loss: 0.6323 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 247/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 955us/step - accuracy: 0.6467 - loss: 0.6496 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 248/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6649 - loss: 0.6381 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 249/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 879us/step - accuracy: 0.6748 - loss: 0.6318 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 250/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 995us/step - accuracy: 0.6630 - loss: 0.6392 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 251/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 838us/step - accuracy: 0.6579 - loss: 0.6425 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 252/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6480 - loss: 0.6488 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 253/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 951us/step - accuracy: 0.6500 - loss: 0.6475 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 254/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6471 - loss: 0.6494 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 255/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 881us/step - accuracy: 0.6450 - loss: 0.6507 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 256/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 844us/step - accuracy: 0.6815 - loss: 0.6276 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 257/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 857us/step - accuracy: 0.6383 - loss: 0.6549 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 258/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 883us/step - accuracy: 0.6514 - loss: 0.6466 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 259/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 804us/step - accuracy: 0.6723 - loss: 0.6334 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 260/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 854us/step - accuracy: 0.6526 - loss: 0.6459 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 261/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 877us/step - accuracy: 0.6257 - loss: 0.6628 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 262/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 842us/step - accuracy: 0.6616 - loss: 0.6402 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 263/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 872us/step - accuracy: 0.6775 - loss: 0.6300 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 264/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 834us/step - accuracy: 0.5989 - loss: 0.6800 - val_accuracy: 0.6429 - val_loss: 0.6519\n",
      "Epoch 265/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 934us/step - accuracy: 0.6507 - loss: 0.6470 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 266/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 850us/step - accuracy: 0.6428 - loss: 0.6520 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 267/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 935us/step - accuracy: 0.6196 - loss: 0.6666 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 268/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 878us/step - accuracy: 0.6335 - loss: 0.6579 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 269/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 893us/step - accuracy: 0.6252 - loss: 0.6633 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 270/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 853us/step - accuracy: 0.6608 - loss: 0.6407 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 271/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 826us/step - accuracy: 0.6661 - loss: 0.6373 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 272/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 821us/step - accuracy: 0.6261 - loss: 0.6627 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 273/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 841us/step - accuracy: 0.6413 - loss: 0.6530 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 274/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 811us/step - accuracy: 0.6192 - loss: 0.6670 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 275/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 833us/step - accuracy: 0.6527 - loss: 0.6458 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 276/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 893us/step - accuracy: 0.6442 - loss: 0.6512 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 277/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 887us/step - accuracy: 0.6650 - loss: 0.6380 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 278/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 929us/step - accuracy: 0.6788 - loss: 0.6293 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 279/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6723 - loss: 0.6333 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 280/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 803us/step - accuracy: 0.6692 - loss: 0.6354 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 281/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 823us/step - accuracy: 0.6755 - loss: 0.6313 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 282/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 854us/step - accuracy: 0.6555 - loss: 0.6440 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 283/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 907us/step - accuracy: 0.6497 - loss: 0.6477 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 284/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 818us/step - accuracy: 0.6314 - loss: 0.6594 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 285/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 828us/step - accuracy: 0.6584 - loss: 0.6422 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 286/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 858us/step - accuracy: 0.6379 - loss: 0.6552 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 287/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6556 - loss: 0.6440 - val_accuracy: 0.6429 - val_loss: 0.6521\n",
      "Epoch 288/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 845us/step - accuracy: 0.6373 - loss: 0.6556 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 289/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 954us/step - accuracy: 0.6662 - loss: 0.6372 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 290/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 991us/step - accuracy: 0.6698 - loss: 0.6350 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 291/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step - accuracy: 0.6281 - loss: 0.6614 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 292/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 820us/step - accuracy: 0.6758 - loss: 0.6312 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 293/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 816us/step - accuracy: 0.6683 - loss: 0.6358 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 294/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 836us/step - accuracy: 0.6728 - loss: 0.6331 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 295/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 858us/step - accuracy: 0.6461 - loss: 0.6499 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 296/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 877us/step - accuracy: 0.6451 - loss: 0.6506 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 297/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 914us/step - accuracy: 0.6820 - loss: 0.6272 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 298/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 914us/step - accuracy: 0.6590 - loss: 0.6418 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 299/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 868us/step - accuracy: 0.6416 - loss: 0.6528 - val_accuracy: 0.6429 - val_loss: 0.6520\n",
      "Epoch 300/300\n",
      "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 920us/step - accuracy: 0.6537 - loss: 0.6451 - val_accuracy: 0.6429 - val_loss: 0.6520\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:56.950004Z",
     "start_time": "2024-07-27T08:12:56.943497Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "288e87e74dcdf2a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m)              │            \u001B[38;5;34m72\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m120\u001B[0m)            │         \u001B[38;5;34m1,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m7,744\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │           \u001B[38;5;34m390\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              │            \u001B[38;5;34m14\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │             \u001B[38;5;34m3\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m27,911\u001B[0m (109.03 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,911</span> (109.03 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m9,303\u001B[0m (36.34 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,303</span> (36.34 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m18,608\u001B[0m (72.69 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,608</span> (72.69 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:56.969145Z",
     "start_time": "2024-07-27T08:12:56.950482Z"
    }
   },
   "cell_type": "code",
   "source": "history.history",
   "id": "8ee8cf820513cc69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946,\n",
       "  0.653094470500946],\n",
       " 'loss': [0.645563542842865,\n",
       "  0.6455504298210144,\n",
       "  0.6456165313720703,\n",
       "  0.6455449461936951,\n",
       "  0.645635187625885,\n",
       "  0.6455726027488708,\n",
       "  0.6457004547119141,\n",
       "  0.6455578207969666,\n",
       "  0.6455528736114502,\n",
       "  0.6456201672554016,\n",
       "  0.6455378532409668,\n",
       "  0.6455460786819458,\n",
       "  0.6455602645874023,\n",
       "  0.6455397009849548,\n",
       "  0.6455435156822205,\n",
       "  0.6455755233764648,\n",
       "  0.6455661058425903,\n",
       "  0.6455365419387817,\n",
       "  0.6455641388893127,\n",
       "  0.6455707550048828,\n",
       "  0.6455972790718079,\n",
       "  0.6456028819084167,\n",
       "  0.6455423831939697,\n",
       "  0.6455785632133484,\n",
       "  0.6455527544021606,\n",
       "  0.6455772519111633,\n",
       "  0.6455626487731934,\n",
       "  0.6455420255661011,\n",
       "  0.6455345153808594,\n",
       "  0.6455618739128113,\n",
       "  0.645548939704895,\n",
       "  0.6455467939376831,\n",
       "  0.6455748081207275,\n",
       "  0.6455621123313904,\n",
       "  0.6455778479576111,\n",
       "  0.6455621719360352,\n",
       "  0.6456009745597839,\n",
       "  0.6455632448196411,\n",
       "  0.6455349922180176,\n",
       "  0.6455537676811218,\n",
       "  0.6455618739128113,\n",
       "  0.6455915570259094,\n",
       "  0.6455296277999878,\n",
       "  0.6455351114273071,\n",
       "  0.645564079284668,\n",
       "  0.6456177234649658,\n",
       "  0.6455476880073547,\n",
       "  0.6455346345901489,\n",
       "  0.6456187963485718,\n",
       "  0.6455560326576233,\n",
       "  0.645555853843689,\n",
       "  0.6455404758453369,\n",
       "  0.6455750465393066,\n",
       "  0.6455543041229248,\n",
       "  0.645563542842865,\n",
       "  0.6455321907997131,\n",
       "  0.6455464959144592,\n",
       "  0.6455442905426025,\n",
       "  0.6456095576286316,\n",
       "  0.6455731391906738,\n",
       "  0.6455387473106384,\n",
       "  0.6455618739128113,\n",
       "  0.6455429792404175,\n",
       "  0.64552903175354,\n",
       "  0.6455830335617065,\n",
       "  0.6455897688865662,\n",
       "  0.6455827355384827,\n",
       "  0.6455507278442383,\n",
       "  0.6455931067466736,\n",
       "  0.645583987236023,\n",
       "  0.6455448865890503,\n",
       "  0.6455338597297668,\n",
       "  0.6455785632133484,\n",
       "  0.6455488204956055,\n",
       "  0.6455812454223633,\n",
       "  0.6456191539764404,\n",
       "  0.6456038355827332,\n",
       "  0.6455487608909607,\n",
       "  0.6455764174461365,\n",
       "  0.6455574631690979,\n",
       "  0.6455534100532532,\n",
       "  0.6455918550491333,\n",
       "  0.6455711126327515,\n",
       "  0.6455307006835938,\n",
       "  0.6456055045127869,\n",
       "  0.6455525755882263,\n",
       "  0.6455531716346741,\n",
       "  0.6455734372138977,\n",
       "  0.645621657371521,\n",
       "  0.6455704569816589,\n",
       "  0.6456063389778137,\n",
       "  0.6455294489860535,\n",
       "  0.645595908164978,\n",
       "  0.6455090641975403,\n",
       "  0.6455320119857788,\n",
       "  0.6455428600311279,\n",
       "  0.6455865502357483,\n",
       "  0.6455579400062561,\n",
       "  0.6455546617507935,\n",
       "  0.6455504894256592,\n",
       "  0.6455427408218384,\n",
       "  0.6455513834953308,\n",
       "  0.6455299258232117,\n",
       "  0.6456055045127869,\n",
       "  0.6455682516098022,\n",
       "  0.6456021666526794,\n",
       "  0.6455852389335632,\n",
       "  0.6455250382423401,\n",
       "  0.645539402961731,\n",
       "  0.6455758213996887,\n",
       "  0.6455903649330139,\n",
       "  0.6455800533294678,\n",
       "  0.6455678343772888,\n",
       "  0.6455642580986023,\n",
       "  0.6455604434013367,\n",
       "  0.6455757021903992,\n",
       "  0.6455647349357605,\n",
       "  0.6455668807029724,\n",
       "  0.6455498337745667,\n",
       "  0.6455598473548889,\n",
       "  0.6455384492874146,\n",
       "  0.6455315351486206,\n",
       "  0.6455553770065308,\n",
       "  0.6455845236778259,\n",
       "  0.6455630660057068,\n",
       "  0.645525336265564,\n",
       "  0.6455642580986023,\n",
       "  0.645601749420166,\n",
       "  0.6455577611923218,\n",
       "  0.6455678939819336,\n",
       "  0.6455868482589722,\n",
       "  0.6455628871917725,\n",
       "  0.645582914352417,\n",
       "  0.6455316543579102,\n",
       "  0.6455559134483337,\n",
       "  0.6455301642417908,\n",
       "  0.6456382274627686,\n",
       "  0.6455168724060059,\n",
       "  0.6456012725830078,\n",
       "  0.6455821394920349,\n",
       "  0.6455838680267334,\n",
       "  0.6455541253089905,\n",
       "  0.6456043124198914,\n",
       "  0.6455392837524414,\n",
       "  0.6455366611480713,\n",
       "  0.64554363489151,\n",
       "  0.6455605626106262,\n",
       "  0.6455680727958679,\n",
       "  0.6455603837966919,\n",
       "  0.6455544829368591,\n",
       "  0.6455550193786621,\n",
       "  0.6455792188644409,\n",
       "  0.6455585956573486,\n",
       "  0.6456040143966675,\n",
       "  0.6455474495887756,\n",
       "  0.6455438137054443,\n",
       "  0.6455588936805725,\n",
       "  0.6455485820770264,\n",
       "  0.6455546021461487,\n",
       "  0.645567774772644,\n",
       "  0.6455764174461365,\n",
       "  0.6455548405647278,\n",
       "  0.6455696225166321,\n",
       "  0.6455298066139221,\n",
       "  0.645555317401886,\n",
       "  0.6455466151237488,\n",
       "  0.6455413103103638,\n",
       "  0.6455522775650024,\n",
       "  0.6455814838409424,\n",
       "  0.6455559730529785,\n",
       "  0.6455628275871277,\n",
       "  0.6455477476119995,\n",
       "  0.6455389261245728,\n",
       "  0.6455488801002502,\n",
       "  0.6456298232078552,\n",
       "  0.6455562710762024,\n",
       "  0.645606279373169,\n",
       "  0.6455424427986145,\n",
       "  0.6455565094947815,\n",
       "  0.645530104637146,\n",
       "  0.6456050276756287,\n",
       "  0.6455559134483337,\n",
       "  0.6455792784690857,\n",
       "  0.6455394625663757,\n",
       "  0.6455370187759399,\n",
       "  0.6455725431442261,\n",
       "  0.6456117630004883,\n",
       "  0.6455585360527039,\n",
       "  0.6455933451652527,\n",
       "  0.6455540060997009,\n",
       "  0.6455538868904114,\n",
       "  0.6455870270729065,\n",
       "  0.6455354690551758,\n",
       "  0.6455482244491577,\n",
       "  0.6455434560775757,\n",
       "  0.6455321311950684,\n",
       "  0.645569920539856,\n",
       "  0.645574152469635,\n",
       "  0.6455534100532532,\n",
       "  0.6455544829368591,\n",
       "  0.6455905437469482,\n",
       "  0.6455625295639038,\n",
       "  0.6455335021018982,\n",
       "  0.6455644369125366,\n",
       "  0.6455984711647034,\n",
       "  0.6455708146095276,\n",
       "  0.6455389261245728,\n",
       "  0.6455745100975037,\n",
       "  0.6455761790275574,\n",
       "  0.6456486582756042,\n",
       "  0.6455967426300049,\n",
       "  0.6455417275428772,\n",
       "  0.645545482635498,\n",
       "  0.6455414891242981,\n",
       "  0.6455291509628296,\n",
       "  0.6455814838409424,\n",
       "  0.6455644965171814,\n",
       "  0.6455665826797485,\n",
       "  0.6455766558647156,\n",
       "  0.6455938816070557,\n",
       "  0.6455782055854797,\n",
       "  0.6455510258674622,\n",
       "  0.6455385088920593,\n",
       "  0.6455464363098145,\n",
       "  0.6455576419830322,\n",
       "  0.645608127117157,\n",
       "  0.6455376148223877,\n",
       "  0.6456782221794128,\n",
       "  0.6455366611480713,\n",
       "  0.6455654501914978,\n",
       "  0.6455670595169067,\n",
       "  0.6455274224281311,\n",
       "  0.6455560922622681,\n",
       "  0.6455357670783997,\n",
       "  0.6455914974212646,\n",
       "  0.6455445289611816,\n",
       "  0.6455312967300415,\n",
       "  0.6455483436584473,\n",
       "  0.6455521583557129,\n",
       "  0.6455432176589966,\n",
       "  0.6455609202384949,\n",
       "  0.6456031203269958,\n",
       "  0.6455373167991638,\n",
       "  0.6455477476119995,\n",
       "  0.6455627679824829,\n",
       "  0.6455501914024353,\n",
       "  0.6455546617507935,\n",
       "  0.645525336265564,\n",
       "  0.6455734968185425,\n",
       "  0.6455429196357727,\n",
       "  0.6455715894699097,\n",
       "  0.6455622911453247,\n",
       "  0.645622968673706,\n",
       "  0.645600438117981,\n",
       "  0.6455488204956055,\n",
       "  0.6455709338188171,\n",
       "  0.645584225654602,\n",
       "  0.6455312967300415,\n",
       "  0.6455479264259338,\n",
       "  0.6456097960472107,\n",
       "  0.6455860137939453,\n",
       "  0.6455636024475098,\n",
       "  0.6456491947174072,\n",
       "  0.645818293094635,\n",
       "  0.6455351710319519,\n",
       "  0.6455784440040588,\n",
       "  0.6455355882644653,\n",
       "  0.6455551385879517,\n",
       "  0.6455987691879272,\n",
       "  0.6455734968185425,\n",
       "  0.6455462574958801,\n",
       "  0.645584225654602,\n",
       "  0.6455504894256592,\n",
       "  0.6455493569374084,\n",
       "  0.6455546617507935,\n",
       "  0.6455931067466736,\n",
       "  0.6455618143081665,\n",
       "  0.6456023454666138,\n",
       "  0.6455917358398438,\n",
       "  0.6455559134483337,\n",
       "  0.64554762840271,\n",
       "  0.6455568671226501,\n",
       "  0.6455574035644531,\n",
       "  0.645557165145874,\n",
       "  0.6455485820770264,\n",
       "  0.6455385684967041,\n",
       "  0.6455972790718079,\n",
       "  0.6455627679824829,\n",
       "  0.6455576419830322,\n",
       "  0.6455429792404175,\n",
       "  0.6455567479133606,\n",
       "  0.6456462144851685,\n",
       "  0.6455373167991638,\n",
       "  0.6455564498901367,\n",
       "  0.6455857753753662,\n",
       "  0.6455507278442383,\n",
       "  0.6455522775650024,\n",
       "  0.6455315947532654,\n",
       "  0.6455455422401428,\n",
       "  0.6455968022346497],\n",
       " 'val_accuracy': [0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936,\n",
       "  0.6428571343421936],\n",
       " 'val_loss': [0.6519707441329956,\n",
       "  0.6519678235054016,\n",
       "  0.6519620418548584,\n",
       "  0.651961088180542,\n",
       "  0.652040421962738,\n",
       "  0.6520196795463562,\n",
       "  0.6520271897315979,\n",
       "  0.6519957780838013,\n",
       "  0.6520153284072876,\n",
       "  0.6520190238952637,\n",
       "  0.652006983757019,\n",
       "  0.6520023941993713,\n",
       "  0.6519966721534729,\n",
       "  0.6519801616668701,\n",
       "  0.6520004868507385,\n",
       "  0.6519959568977356,\n",
       "  0.6519771218299866,\n",
       "  0.6519773602485657,\n",
       "  0.6519517302513123,\n",
       "  0.6519779562950134,\n",
       "  0.6519802212715149,\n",
       "  0.6519727110862732,\n",
       "  0.65202397108078,\n",
       "  0.6519907116889954,\n",
       "  0.6519979238510132,\n",
       "  0.6520532965660095,\n",
       "  0.6520191431045532,\n",
       "  0.652001678943634,\n",
       "  0.6519890427589417,\n",
       "  0.6519794464111328,\n",
       "  0.6519913673400879,\n",
       "  0.651982843875885,\n",
       "  0.6519868969917297,\n",
       "  0.6520231366157532,\n",
       "  0.6520374417304993,\n",
       "  0.6520546674728394,\n",
       "  0.6520553827285767,\n",
       "  0.6520504355430603,\n",
       "  0.6520413756370544,\n",
       "  0.652008056640625,\n",
       "  0.6520071625709534,\n",
       "  0.6520558595657349,\n",
       "  0.6520109176635742,\n",
       "  0.6519983410835266,\n",
       "  0.6519902348518372,\n",
       "  0.652002215385437,\n",
       "  0.6520146727561951,\n",
       "  0.6519631743431091,\n",
       "  0.6519460082054138,\n",
       "  0.6519617438316345,\n",
       "  0.6519824266433716,\n",
       "  0.6519619822502136,\n",
       "  0.6519790291786194,\n",
       "  0.6519564390182495,\n",
       "  0.6519818305969238,\n",
       "  0.6519584059715271,\n",
       "  0.6519433259963989,\n",
       "  0.6519486308097839,\n",
       "  0.6519513726234436,\n",
       "  0.6519854664802551,\n",
       "  0.6519583463668823,\n",
       "  0.6519458293914795,\n",
       "  0.6519718766212463,\n",
       "  0.6519705653190613,\n",
       "  0.6520060896873474,\n",
       "  0.6519662141799927,\n",
       "  0.6519999504089355,\n",
       "  0.6519944071769714,\n",
       "  0.6519639492034912,\n",
       "  0.6519721150398254,\n",
       "  0.6519790291786194,\n",
       "  0.6519535183906555,\n",
       "  0.6519745588302612,\n",
       "  0.6519608497619629,\n",
       "  0.6519629955291748,\n",
       "  0.6519603133201599,\n",
       "  0.6519852876663208,\n",
       "  0.6519585847854614,\n",
       "  0.6519988775253296,\n",
       "  0.6519619822502136,\n",
       "  0.6519984006881714,\n",
       "  0.6519941091537476,\n",
       "  0.6519826650619507,\n",
       "  0.6519920229911804,\n",
       "  0.6519710421562195,\n",
       "  0.6519812345504761,\n",
       "  0.6519731879234314,\n",
       "  0.6519509553909302,\n",
       "  0.6519642472267151,\n",
       "  0.651971697807312,\n",
       "  0.6519796848297119,\n",
       "  0.6519615650177002,\n",
       "  0.652005136013031,\n",
       "  0.6519685983657837,\n",
       "  0.6519392728805542,\n",
       "  0.6519434452056885,\n",
       "  0.6519764065742493,\n",
       "  0.6519919633865356,\n",
       "  0.651971697807312,\n",
       "  0.6519500017166138,\n",
       "  0.6519801616668701,\n",
       "  0.6519463062286377,\n",
       "  0.6519597768783569,\n",
       "  0.6519748568534851,\n",
       "  0.6519626975059509,\n",
       "  0.6520349383354187,\n",
       "  0.6519609689712524,\n",
       "  0.6520106196403503,\n",
       "  0.6520355939865112,\n",
       "  0.6520017385482788,\n",
       "  0.6520208120346069,\n",
       "  0.6520284414291382,\n",
       "  0.6519950032234192,\n",
       "  0.6519792079925537,\n",
       "  0.6520067453384399,\n",
       "  0.6520029902458191,\n",
       "  0.6519824266433716,\n",
       "  0.6519878506660461,\n",
       "  0.6519819498062134,\n",
       "  0.6519806385040283,\n",
       "  0.6519942283630371,\n",
       "  0.6520209312438965,\n",
       "  0.6520228981971741,\n",
       "  0.6520187854766846,\n",
       "  0.6520091891288757,\n",
       "  0.6520352959632874,\n",
       "  0.6520004868507385,\n",
       "  0.6519728899002075,\n",
       "  0.6520382165908813,\n",
       "  0.6520411968231201,\n",
       "  0.6520563364028931,\n",
       "  0.6520097255706787,\n",
       "  0.6520483493804932,\n",
       "  0.6520386934280396,\n",
       "  0.65199875831604,\n",
       "  0.6520017385482788,\n",
       "  0.6519617438316345,\n",
       "  0.6520124673843384,\n",
       "  0.6519948244094849,\n",
       "  0.6519894003868103,\n",
       "  0.6519938707351685,\n",
       "  0.6519971489906311,\n",
       "  0.6520295739173889,\n",
       "  0.6519904732704163,\n",
       "  0.6519910097122192,\n",
       "  0.6519719362258911,\n",
       "  0.6519722938537598,\n",
       "  0.6519825458526611,\n",
       "  0.6519850492477417,\n",
       "  0.6520162224769592,\n",
       "  0.6519932150840759,\n",
       "  0.6519922614097595,\n",
       "  0.6519979238510132,\n",
       "  0.6519722938537598,\n",
       "  0.6519901752471924,\n",
       "  0.6520317792892456,\n",
       "  0.6520395278930664,\n",
       "  0.6520277261734009,\n",
       "  0.6519958972930908,\n",
       "  0.6519947052001953,\n",
       "  0.6519709825515747,\n",
       "  0.6520218253135681,\n",
       "  0.6520277261734009,\n",
       "  0.651991605758667,\n",
       "  0.6519776582717896,\n",
       "  0.6519877910614014,\n",
       "  0.6519872546195984,\n",
       "  0.6519725322723389,\n",
       "  0.6519644856452942,\n",
       "  0.6519735455513,\n",
       "  0.6519941091537476,\n",
       "  0.652002215385437,\n",
       "  0.6519728302955627,\n",
       "  0.6519762277603149,\n",
       "  0.6520284414291382,\n",
       "  0.6519829630851746,\n",
       "  0.6519883275032043,\n",
       "  0.6519892811775208,\n",
       "  0.6519685983657837,\n",
       "  0.651971697807312,\n",
       "  0.6520107984542847,\n",
       "  0.6520018577575684,\n",
       "  0.6519536375999451,\n",
       "  0.6519765853881836,\n",
       "  0.6519727110862732,\n",
       "  0.6519567370414734,\n",
       "  0.6519961953163147,\n",
       "  0.6519599556922913,\n",
       "  0.651996374130249,\n",
       "  0.6519617438316345,\n",
       "  0.6519435048103333,\n",
       "  0.6520026326179504,\n",
       "  0.6519804000854492,\n",
       "  0.6519985198974609,\n",
       "  0.6519888639450073,\n",
       "  0.6519845128059387,\n",
       "  0.6520155668258667,\n",
       "  0.6519740223884583,\n",
       "  0.6519820690155029,\n",
       "  0.6520107388496399,\n",
       "  0.652027428150177,\n",
       "  0.6520099639892578,\n",
       "  0.6520169973373413,\n",
       "  0.652023196220398,\n",
       "  0.652006983757019,\n",
       "  0.6520116329193115,\n",
       "  0.6519941687583923,\n",
       "  0.6520077586174011,\n",
       "  0.6519832015037537,\n",
       "  0.6520028114318848,\n",
       "  0.6519654393196106,\n",
       "  0.6520031094551086,\n",
       "  0.6519768238067627,\n",
       "  0.6519888639450073,\n",
       "  0.65199214220047,\n",
       "  0.6520105004310608,\n",
       "  0.6520139575004578,\n",
       "  0.6520006060600281,\n",
       "  0.6519975662231445,\n",
       "  0.6519843339920044,\n",
       "  0.6520093083381653,\n",
       "  0.6520105600357056,\n",
       "  0.6519800424575806,\n",
       "  0.6519859433174133,\n",
       "  0.6520068049430847,\n",
       "  0.6520180106163025,\n",
       "  0.651971161365509,\n",
       "  0.6519846320152283,\n",
       "  0.6519649028778076,\n",
       "  0.6519769430160522,\n",
       "  0.6519629955291748,\n",
       "  0.6519486308097839,\n",
       "  0.6519467234611511,\n",
       "  0.6519863605499268,\n",
       "  0.651968240737915,\n",
       "  0.6519663333892822,\n",
       "  0.6519782543182373,\n",
       "  0.6519847512245178,\n",
       "  0.6520155072212219,\n",
       "  0.6519813537597656,\n",
       "  0.6520164012908936,\n",
       "  0.6520398259162903,\n",
       "  0.6519964933395386,\n",
       "  0.651997983455658,\n",
       "  0.6520075798034668,\n",
       "  0.6520048975944519,\n",
       "  0.6519995331764221,\n",
       "  0.6520001888275146,\n",
       "  0.6520276665687561,\n",
       "  0.6519794464111328,\n",
       "  0.6519574522972107,\n",
       "  0.6519851684570312,\n",
       "  0.6520196795463562,\n",
       "  0.6520443558692932,\n",
       "  0.6519778370857239,\n",
       "  0.6520060896873474,\n",
       "  0.6519752740859985,\n",
       "  0.6519801020622253,\n",
       "  0.6519714593887329,\n",
       "  0.6519477367401123,\n",
       "  0.6519455909729004,\n",
       "  0.6519889235496521,\n",
       "  0.6520681381225586,\n",
       "  0.6519293189048767,\n",
       "  0.6520075798034668,\n",
       "  0.6519509553909302,\n",
       "  0.6519966721534729,\n",
       "  0.6520100235939026,\n",
       "  0.6519788503646851,\n",
       "  0.6519845128059387,\n",
       "  0.6520199179649353,\n",
       "  0.6519973278045654,\n",
       "  0.6519879698753357,\n",
       "  0.6519991159439087,\n",
       "  0.652032196521759,\n",
       "  0.6520065665245056,\n",
       "  0.6520066857337952,\n",
       "  0.6520131826400757,\n",
       "  0.6519867777824402,\n",
       "  0.6520190238952637,\n",
       "  0.6520200371742249,\n",
       "  0.6519827246665955,\n",
       "  0.6520226001739502,\n",
       "  0.6520082354545593,\n",
       "  0.6519885063171387,\n",
       "  0.6520184874534607,\n",
       "  0.6520501375198364,\n",
       "  0.6519845128059387,\n",
       "  0.6520255208015442,\n",
       "  0.6519809365272522,\n",
       "  0.6519968509674072,\n",
       "  0.6520342826843262,\n",
       "  0.6519907712936401,\n",
       "  0.6520155668258667,\n",
       "  0.6519666314125061,\n",
       "  0.6519858241081238,\n",
       "  0.6519933938980103,\n",
       "  0.6519869565963745,\n",
       "  0.6519754528999329,\n",
       "  0.6519701480865479]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot the accuracy",
   "id": "18596c7aad9ff3f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.201343Z",
     "start_time": "2024-07-27T08:12:56.969965Z"
    }
   },
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt",
   "id": "9557eb006b25c490",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.299737Z",
     "start_time": "2024-07-27T08:12:57.202154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ],
   "id": "7010dc4d844a0d53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f882789d1d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DNN - Regression\n",
    "* We will use the king county house sales dataset.\n",
    "* We will predict the price of the house.\n",
    "* We will use the following columns to predict the price of the house.\n",
    "  * bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, yr_built, yr_renovated, zipcode, lat, long, sqft_living15, sqft_lot15"
   ],
   "id": "fdfdb5038bae81b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting the price of the house",
   "id": "cddcc170f1a5362"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.310036Z",
     "start_time": "2024-07-27T08:12:57.300323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_pickle('kc_house.pkl') # this is a feature engineered dataset\n",
    "df.head()\n"
   ],
   "id": "f41bf62f9dbaf3e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   bedrooms  bathrooms  sqft_living  grade  view  basement  waterfront  \\\n",
       "0         9     1.0000         1180      7     0         0           0   \n",
       "1         9     5.0625         2570      7     0         1           0   \n",
       "2         4     1.0000          770      6     0         0           0   \n",
       "3        16     9.0000         1960      7     0         1           0   \n",
       "4         9     4.0000         1680      8     0         0           0   \n",
       "\n",
       "   floors  age  renovated  ...  zipcode_98146  zipcode_98148  zipcode_98155  \\\n",
       "0     1.0   65          0  ...              0              0              0   \n",
       "1     2.0   69          1  ...              0              0              0   \n",
       "2     1.0   87          0  ...              0              0              0   \n",
       "3     1.0   55          0  ...              0              0              0   \n",
       "4     1.0   33          0  ...              0              0              0   \n",
       "\n",
       "   zipcode_98166  zipcode_98168  zipcode_98177  zipcode_98178  zipcode_98188  \\\n",
       "0              0              0              0              1              0   \n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   zipcode_98198  zipcode_98199  \n",
       "0              0              0  \n",
       "1              0              0  \n",
       "2              0              0  \n",
       "3              0              0  \n",
       "4              0              0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>grade</th>\n",
       "      <th>view</th>\n",
       "      <th>basement</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>floors</th>\n",
       "      <th>age</th>\n",
       "      <th>renovated</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_98146</th>\n",
       "      <th>zipcode_98148</th>\n",
       "      <th>zipcode_98155</th>\n",
       "      <th>zipcode_98166</th>\n",
       "      <th>zipcode_98168</th>\n",
       "      <th>zipcode_98177</th>\n",
       "      <th>zipcode_98178</th>\n",
       "      <th>zipcode_98188</th>\n",
       "      <th>zipcode_98198</th>\n",
       "      <th>zipcode_98199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1180</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>5.0625</td>\n",
       "      <td>2570</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>770</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>1960</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1680</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.316797Z",
     "start_time": "2024-07-27T08:12:57.310573Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "4c7c5f805671ba9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19034 entries, 0 to 21612\n",
      "Data columns (total 82 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   bedrooms       19034 non-null  int64  \n",
      " 1   bathrooms      19034 non-null  float64\n",
      " 2   sqft_living    19034 non-null  int64  \n",
      " 3   grade          19034 non-null  int64  \n",
      " 4   view           19034 non-null  int64  \n",
      " 5   basement       19034 non-null  int64  \n",
      " 6   waterfront     19034 non-null  int64  \n",
      " 7   floors         19034 non-null  float64\n",
      " 8   age            19034 non-null  int64  \n",
      " 9   renovated      19034 non-null  int64  \n",
      " 10  condition      19034 non-null  int64  \n",
      " 11  sqft_above     19034 non-null  int64  \n",
      " 12  price          19034 non-null  float64\n",
      " 13  zipcode_98002  19034 non-null  uint8  \n",
      " 14  zipcode_98003  19034 non-null  uint8  \n",
      " 15  zipcode_98004  19034 non-null  uint8  \n",
      " 16  zipcode_98005  19034 non-null  uint8  \n",
      " 17  zipcode_98006  19034 non-null  uint8  \n",
      " 18  zipcode_98007  19034 non-null  uint8  \n",
      " 19  zipcode_98008  19034 non-null  uint8  \n",
      " 20  zipcode_98010  19034 non-null  uint8  \n",
      " 21  zipcode_98011  19034 non-null  uint8  \n",
      " 22  zipcode_98014  19034 non-null  uint8  \n",
      " 23  zipcode_98019  19034 non-null  uint8  \n",
      " 24  zipcode_98022  19034 non-null  uint8  \n",
      " 25  zipcode_98023  19034 non-null  uint8  \n",
      " 26  zipcode_98024  19034 non-null  uint8  \n",
      " 27  zipcode_98027  19034 non-null  uint8  \n",
      " 28  zipcode_98028  19034 non-null  uint8  \n",
      " 29  zipcode_98029  19034 non-null  uint8  \n",
      " 30  zipcode_98030  19034 non-null  uint8  \n",
      " 31  zipcode_98031  19034 non-null  uint8  \n",
      " 32  zipcode_98032  19034 non-null  uint8  \n",
      " 33  zipcode_98033  19034 non-null  uint8  \n",
      " 34  zipcode_98034  19034 non-null  uint8  \n",
      " 35  zipcode_98038  19034 non-null  uint8  \n",
      " 36  zipcode_98039  19034 non-null  uint8  \n",
      " 37  zipcode_98040  19034 non-null  uint8  \n",
      " 38  zipcode_98042  19034 non-null  uint8  \n",
      " 39  zipcode_98045  19034 non-null  uint8  \n",
      " 40  zipcode_98052  19034 non-null  uint8  \n",
      " 41  zipcode_98053  19034 non-null  uint8  \n",
      " 42  zipcode_98055  19034 non-null  uint8  \n",
      " 43  zipcode_98056  19034 non-null  uint8  \n",
      " 44  zipcode_98058  19034 non-null  uint8  \n",
      " 45  zipcode_98059  19034 non-null  uint8  \n",
      " 46  zipcode_98065  19034 non-null  uint8  \n",
      " 47  zipcode_98070  19034 non-null  uint8  \n",
      " 48  zipcode_98072  19034 non-null  uint8  \n",
      " 49  zipcode_98074  19034 non-null  uint8  \n",
      " 50  zipcode_98075  19034 non-null  uint8  \n",
      " 51  zipcode_98077  19034 non-null  uint8  \n",
      " 52  zipcode_98092  19034 non-null  uint8  \n",
      " 53  zipcode_98102  19034 non-null  uint8  \n",
      " 54  zipcode_98103  19034 non-null  uint8  \n",
      " 55  zipcode_98105  19034 non-null  uint8  \n",
      " 56  zipcode_98106  19034 non-null  uint8  \n",
      " 57  zipcode_98107  19034 non-null  uint8  \n",
      " 58  zipcode_98108  19034 non-null  uint8  \n",
      " 59  zipcode_98109  19034 non-null  uint8  \n",
      " 60  zipcode_98112  19034 non-null  uint8  \n",
      " 61  zipcode_98115  19034 non-null  uint8  \n",
      " 62  zipcode_98116  19034 non-null  uint8  \n",
      " 63  zipcode_98117  19034 non-null  uint8  \n",
      " 64  zipcode_98118  19034 non-null  uint8  \n",
      " 65  zipcode_98119  19034 non-null  uint8  \n",
      " 66  zipcode_98122  19034 non-null  uint8  \n",
      " 67  zipcode_98125  19034 non-null  uint8  \n",
      " 68  zipcode_98126  19034 non-null  uint8  \n",
      " 69  zipcode_98133  19034 non-null  uint8  \n",
      " 70  zipcode_98136  19034 non-null  uint8  \n",
      " 71  zipcode_98144  19034 non-null  uint8  \n",
      " 72  zipcode_98146  19034 non-null  uint8  \n",
      " 73  zipcode_98148  19034 non-null  uint8  \n",
      " 74  zipcode_98155  19034 non-null  uint8  \n",
      " 75  zipcode_98166  19034 non-null  uint8  \n",
      " 76  zipcode_98168  19034 non-null  uint8  \n",
      " 77  zipcode_98177  19034 non-null  uint8  \n",
      " 78  zipcode_98178  19034 non-null  uint8  \n",
      " 79  zipcode_98188  19034 non-null  uint8  \n",
      " 80  zipcode_98198  19034 non-null  uint8  \n",
      " 81  zipcode_98199  19034 non-null  uint8  \n",
      "dtypes: float64(3), int64(10), uint8(69)\n",
      "memory usage: 3.3 MB\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.383577Z",
     "start_time": "2024-07-27T08:12:57.317276Z"
    }
   },
   "cell_type": "code",
   "source": "df.describe()",
   "id": "9a86b4b8fb1bf8e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           bedrooms     bathrooms   sqft_living         grade          view  \\\n",
       "count  19034.000000  19034.000000  19034.000000  19034.000000  19034.000000   \n",
       "mean      10.539298      4.460672   1904.438268      7.513607      0.178733   \n",
       "std        4.291567      2.775137    712.456490      1.039517      0.658054   \n",
       "min        0.000000      0.000000    290.000000      1.000000      0.000000   \n",
       "25%        9.000000      2.250000   1370.000000      7.000000      0.000000   \n",
       "50%        9.000000      4.000000   1800.000000      7.000000      0.000000   \n",
       "75%       16.000000      6.250000   2350.000000      8.000000      0.000000   \n",
       "max       16.000000     25.000000   4133.000000     12.000000      4.000000   \n",
       "\n",
       "           basement    waterfront        floors           age     renovated  \\\n",
       "count  19034.000000  19034.000000  19034.000000  19034.000000  19034.000000   \n",
       "mean       0.361984      0.003993      1.473022     49.430388      0.038352   \n",
       "std        0.480587      0.063064      0.541476     29.231334      0.192051   \n",
       "min        0.000000      0.000000      1.000000      5.000000      0.000000   \n",
       "25%        0.000000      0.000000      1.000000     24.000000      0.000000   \n",
       "50%        0.000000      0.000000      1.000000     46.000000      0.000000   \n",
       "75%        1.000000      0.000000      2.000000     69.000000      0.000000   \n",
       "max        1.000000      1.000000      3.500000    120.000000      1.000000   \n",
       "\n",
       "       ...  zipcode_98146  zipcode_98148  zipcode_98155  zipcode_98166  \\\n",
       "count  ...   19034.000000   19034.000000   19034.000000   19034.000000   \n",
       "mean   ...       0.014028       0.002942       0.020910       0.011663   \n",
       "std    ...       0.117607       0.054163       0.143087       0.107368   \n",
       "min    ...       0.000000       0.000000       0.000000       0.000000   \n",
       "25%    ...       0.000000       0.000000       0.000000       0.000000   \n",
       "50%    ...       0.000000       0.000000       0.000000       0.000000   \n",
       "75%    ...       0.000000       0.000000       0.000000       0.000000   \n",
       "max    ...       1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       zipcode_98168  zipcode_98177  zipcode_98178  zipcode_98188  \\\n",
       "count   19034.000000   19034.000000   19034.000000   19034.000000   \n",
       "mean        0.013134       0.011401       0.012241       0.006357   \n",
       "std         0.113853       0.106166       0.109964       0.079479   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       zipcode_98198  zipcode_98199  \n",
       "count   19034.000000   19034.000000  \n",
       "mean        0.013765       0.014711  \n",
       "std         0.116516       0.120395  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  \n",
       "\n",
       "[8 rows x 82 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>grade</th>\n",
       "      <th>view</th>\n",
       "      <th>basement</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>floors</th>\n",
       "      <th>age</th>\n",
       "      <th>renovated</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_98146</th>\n",
       "      <th>zipcode_98148</th>\n",
       "      <th>zipcode_98155</th>\n",
       "      <th>zipcode_98166</th>\n",
       "      <th>zipcode_98168</th>\n",
       "      <th>zipcode_98177</th>\n",
       "      <th>zipcode_98178</th>\n",
       "      <th>zipcode_98188</th>\n",
       "      <th>zipcode_98198</th>\n",
       "      <th>zipcode_98199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "      <td>19034.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.539298</td>\n",
       "      <td>4.460672</td>\n",
       "      <td>1904.438268</td>\n",
       "      <td>7.513607</td>\n",
       "      <td>0.178733</td>\n",
       "      <td>0.361984</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>1.473022</td>\n",
       "      <td>49.430388</td>\n",
       "      <td>0.038352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.011663</td>\n",
       "      <td>0.013134</td>\n",
       "      <td>0.011401</td>\n",
       "      <td>0.012241</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>0.013765</td>\n",
       "      <td>0.014711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.291567</td>\n",
       "      <td>2.775137</td>\n",
       "      <td>712.456490</td>\n",
       "      <td>1.039517</td>\n",
       "      <td>0.658054</td>\n",
       "      <td>0.480587</td>\n",
       "      <td>0.063064</td>\n",
       "      <td>0.541476</td>\n",
       "      <td>29.231334</td>\n",
       "      <td>0.192051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117607</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.143087</td>\n",
       "      <td>0.107368</td>\n",
       "      <td>0.113853</td>\n",
       "      <td>0.106166</td>\n",
       "      <td>0.109964</td>\n",
       "      <td>0.079479</td>\n",
       "      <td>0.116516</td>\n",
       "      <td>0.120395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1370.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>2350.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>4133.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 82 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.387943Z",
     "start_time": "2024-07-27T08:12:57.384316Z"
    }
   },
   "cell_type": "code",
   "source": "df.isnull().sum()",
   "id": "3aaa4eee3929758c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bedrooms         0\n",
       "bathrooms        0\n",
       "sqft_living      0\n",
       "grade            0\n",
       "view             0\n",
       "                ..\n",
       "zipcode_98177    0\n",
       "zipcode_98178    0\n",
       "zipcode_98188    0\n",
       "zipcode_98198    0\n",
       "zipcode_98199    0\n",
       "Length: 82, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create the model",
   "id": "e443102765fbb1fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.404417Z",
     "start_time": "2024-07-27T08:12:57.388471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df['price']"
   ],
   "id": "3201b08e5756e256",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.427363Z",
     "start_time": "2024-07-27T08:12:57.404982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "c73b5bd273660f23",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:12:57.434179Z",
     "start_time": "2024-07-27T08:12:57.427994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam') # remove accuracy because this is regression problem, we will use mean_squared_error\n",
    "\n",
    "\n"
   ],
   "id": "7d2b58e8575666e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:15:38.271467Z",
     "start_time": "2024-07-27T08:12:57.435705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1500, batch_size=128, verbose=0)"
   ],
   "id": "3e0a22be940cb96d",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate the r2\n",
   "id": "8ffb063384a400ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:15:38.416902Z",
     "start_time": "2024-07-27T08:15:38.272364Z"
    }
   },
   "cell_type": "code",
   "source": "pred = model.predict(X_test)\n",
   "id": "94ae7f425c6377aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m119/119\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 570us/step\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:15:38.420475Z",
     "start_time": "2024-07-27T08:15:38.417672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "r2_score(y_test, pred), mean_squared_error(y_test, pred)**0.5"
   ],
   "id": "5e7c7f6c60e29684",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.83907807383205, 90761.81220679921)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:15:38.450101Z",
     "start_time": "2024-07-27T08:15:38.420969Z"
    }
   },
   "cell_type": "code",
   "source": "history.history",
   "id": "246522b29f96300b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [245863596032.0,\n",
       "  42328113152.0,\n",
       "  33273702400.0,\n",
       "  32853958656.0,\n",
       "  32490493952.0,\n",
       "  32086376448.0,\n",
       "  31658184704.0,\n",
       "  31310444544.0,\n",
       "  30922508288.0,\n",
       "  30528882688.0,\n",
       "  30205368320.0,\n",
       "  29832781824.0,\n",
       "  29529290752.0,\n",
       "  29222776832.0,\n",
       "  28975323136.0,\n",
       "  28734449664.0,\n",
       "  28505477120.0,\n",
       "  28404647936.0,\n",
       "  28244463616.0,\n",
       "  28163768320.0,\n",
       "  28065781760.0,\n",
       "  28002582528.0,\n",
       "  27951511552.0,\n",
       "  27897149440.0,\n",
       "  27864088576.0,\n",
       "  27784554496.0,\n",
       "  27738060800.0,\n",
       "  27694376960.0,\n",
       "  27654340608.0,\n",
       "  27551657984.0,\n",
       "  27492286464.0,\n",
       "  27476940800.0,\n",
       "  27431084032.0,\n",
       "  27359420416.0,\n",
       "  27265323008.0,\n",
       "  27229999104.0,\n",
       "  27138340864.0,\n",
       "  27134294016.0,\n",
       "  26965790720.0,\n",
       "  26934638592.0,\n",
       "  26875099136.0,\n",
       "  26741686272.0,\n",
       "  26632740864.0,\n",
       "  26575974400.0,\n",
       "  26412128256.0,\n",
       "  26242498560.0,\n",
       "  26035843072.0,\n",
       "  25881542656.0,\n",
       "  25698981888.0,\n",
       "  25437403136.0,\n",
       "  25199745024.0,\n",
       "  24825864192.0,\n",
       "  24533213184.0,\n",
       "  24080504832.0,\n",
       "  23602644992.0,\n",
       "  23152347136.0,\n",
       "  22561576960.0,\n",
       "  21958119424.0,\n",
       "  21387120640.0,\n",
       "  20652324864.0,\n",
       "  20129060864.0,\n",
       "  19645069312.0,\n",
       "  19188932608.0,\n",
       "  18774173696.0,\n",
       "  18535405568.0,\n",
       "  18097131520.0,\n",
       "  17882437632.0,\n",
       "  17574707200.0,\n",
       "  17439891456.0,\n",
       "  17160542208.0,\n",
       "  16871804928.0,\n",
       "  16746959872.0,\n",
       "  16339323904.0,\n",
       "  16256773120.0,\n",
       "  16085063680.0,\n",
       "  15811179520.0,\n",
       "  15619154944.0,\n",
       "  15250509824.0,\n",
       "  15130787840.0,\n",
       "  14991397888.0,\n",
       "  14778810368.0,\n",
       "  14628668416.0,\n",
       "  14350217216.0,\n",
       "  14273310720.0,\n",
       "  13932867584.0,\n",
       "  13817766912.0,\n",
       "  13673618432.0,\n",
       "  13704889344.0,\n",
       "  13570462720.0,\n",
       "  13229872128.0,\n",
       "  13151555584.0,\n",
       "  13046029312.0,\n",
       "  12845331456.0,\n",
       "  12799650816.0,\n",
       "  12620811264.0,\n",
       "  12447157248.0,\n",
       "  12419909632.0,\n",
       "  12359223296.0,\n",
       "  12171608064.0,\n",
       "  12077720576.0,\n",
       "  12029023232.0,\n",
       "  12003714048.0,\n",
       "  11887566848.0,\n",
       "  11845970944.0,\n",
       "  11772874752.0,\n",
       "  11603250176.0,\n",
       "  11556065280.0,\n",
       "  11686707200.0,\n",
       "  11427300352.0,\n",
       "  11400014848.0,\n",
       "  11418515456.0,\n",
       "  11349600256.0,\n",
       "  11450089472.0,\n",
       "  11301133312.0,\n",
       "  11238092800.0,\n",
       "  11153379328.0,\n",
       "  11136273408.0,\n",
       "  10997111808.0,\n",
       "  11195628544.0,\n",
       "  10999924736.0,\n",
       "  10943144960.0,\n",
       "  10863414272.0,\n",
       "  10868953088.0,\n",
       "  10920652800.0,\n",
       "  10798757888.0,\n",
       "  10869650432.0,\n",
       "  10786154496.0,\n",
       "  10671386624.0,\n",
       "  10650097664.0,\n",
       "  10739767296.0,\n",
       "  10813457408.0,\n",
       "  10651571200.0,\n",
       "  10611202048.0,\n",
       "  10703512576.0,\n",
       "  10551797760.0,\n",
       "  10531986432.0,\n",
       "  10554080256.0,\n",
       "  10527791104.0,\n",
       "  10447285248.0,\n",
       "  10411013120.0,\n",
       "  10369257472.0,\n",
       "  10412666880.0,\n",
       "  10328554496.0,\n",
       "  10576509952.0,\n",
       "  10350584832.0,\n",
       "  10407027712.0,\n",
       "  10471180288.0,\n",
       "  10347527168.0,\n",
       "  10452259840.0,\n",
       "  10441961472.0,\n",
       "  10306774016.0,\n",
       "  10298174464.0,\n",
       "  10239186944.0,\n",
       "  10327233536.0,\n",
       "  10204107776.0,\n",
       "  10187956224.0,\n",
       "  10268126208.0,\n",
       "  10288088064.0,\n",
       "  10242455552.0,\n",
       "  10187800576.0,\n",
       "  10074966016.0,\n",
       "  10049993728.0,\n",
       "  10070252544.0,\n",
       "  10341124096.0,\n",
       "  10136246272.0,\n",
       "  10158833664.0,\n",
       "  10206958592.0,\n",
       "  10109296640.0,\n",
       "  10009528320.0,\n",
       "  10085351424.0,\n",
       "  9971374080.0,\n",
       "  9929719808.0,\n",
       "  9944435712.0,\n",
       "  10197113856.0,\n",
       "  10119972864.0,\n",
       "  10016521216.0,\n",
       "  9910946816.0,\n",
       "  9957592064.0,\n",
       "  9907558400.0,\n",
       "  9953765376.0,\n",
       "  9940269056.0,\n",
       "  9912297472.0,\n",
       "  9982688256.0,\n",
       "  9919972352.0,\n",
       "  9987316736.0,\n",
       "  9973320704.0,\n",
       "  9851608064.0,\n",
       "  9917142016.0,\n",
       "  9875863552.0,\n",
       "  10094440448.0,\n",
       "  9852404736.0,\n",
       "  9768284160.0,\n",
       "  9950916608.0,\n",
       "  10220350464.0,\n",
       "  9806709760.0,\n",
       "  9847705600.0,\n",
       "  9985608704.0,\n",
       "  9799095296.0,\n",
       "  9728299008.0,\n",
       "  9838113792.0,\n",
       "  9720368128.0,\n",
       "  9768281088.0,\n",
       "  9777062912.0,\n",
       "  9753284608.0,\n",
       "  9711415296.0,\n",
       "  9748590592.0,\n",
       "  9917356032.0,\n",
       "  9736511488.0,\n",
       "  9726846976.0,\n",
       "  9715434496.0,\n",
       "  9837435904.0,\n",
       "  9716531200.0,\n",
       "  9870048256.0,\n",
       "  9813648384.0,\n",
       "  9879416832.0,\n",
       "  9723987968.0,\n",
       "  9640260608.0,\n",
       "  9669660672.0,\n",
       "  9638170624.0,\n",
       "  9713479680.0,\n",
       "  9711967232.0,\n",
       "  9598411776.0,\n",
       "  9747930112.0,\n",
       "  9640734720.0,\n",
       "  9783148544.0,\n",
       "  9735190528.0,\n",
       "  9596433408.0,\n",
       "  9654102016.0,\n",
       "  9641188352.0,\n",
       "  9851719680.0,\n",
       "  9676918784.0,\n",
       "  9697208320.0,\n",
       "  9611667456.0,\n",
       "  9588174848.0,\n",
       "  9538651136.0,\n",
       "  9639735296.0,\n",
       "  9612883968.0,\n",
       "  9620448256.0,\n",
       "  9578226688.0,\n",
       "  9566552064.0,\n",
       "  9826282496.0,\n",
       "  9568235520.0,\n",
       "  9480963072.0,\n",
       "  9494336512.0,\n",
       "  9602419712.0,\n",
       "  9629980672.0,\n",
       "  9727671296.0,\n",
       "  9528503296.0,\n",
       "  9622843392.0,\n",
       "  9493164032.0,\n",
       "  9677967360.0,\n",
       "  9589690368.0,\n",
       "  9844617216.0,\n",
       "  9597925376.0,\n",
       "  9589056512.0,\n",
       "  9507755008.0,\n",
       "  9507183616.0,\n",
       "  9476974592.0,\n",
       "  9544219648.0,\n",
       "  9591666688.0,\n",
       "  9562375168.0,\n",
       "  9716858880.0,\n",
       "  9506466816.0,\n",
       "  9495814144.0,\n",
       "  9495720960.0,\n",
       "  9651571712.0,\n",
       "  9405991936.0,\n",
       "  9475245056.0,\n",
       "  9464125440.0,\n",
       "  9436158976.0,\n",
       "  9694726144.0,\n",
       "  9530228736.0,\n",
       "  9548339200.0,\n",
       "  9568346112.0,\n",
       "  9475459072.0,\n",
       "  9461929984.0,\n",
       "  9405113344.0,\n",
       "  9538418688.0,\n",
       "  9528741888.0,\n",
       "  9596990464.0,\n",
       "  9389885440.0,\n",
       "  9506704384.0,\n",
       "  9503719424.0,\n",
       "  9597770752.0,\n",
       "  9437868032.0,\n",
       "  9354331136.0,\n",
       "  9483055104.0,\n",
       "  9424540672.0,\n",
       "  9432155136.0,\n",
       "  9580026880.0,\n",
       "  9396793344.0,\n",
       "  9382541312.0,\n",
       "  9478985728.0,\n",
       "  9313142784.0,\n",
       "  9441380352.0,\n",
       "  9477906432.0,\n",
       "  9545210880.0,\n",
       "  9557136384.0,\n",
       "  9469205504.0,\n",
       "  9395488768.0,\n",
       "  9342419968.0,\n",
       "  9402263552.0,\n",
       "  9524343808.0,\n",
       "  9432733696.0,\n",
       "  9343708160.0,\n",
       "  9290063872.0,\n",
       "  9562365952.0,\n",
       "  9395185664.0,\n",
       "  9308748800.0,\n",
       "  9472473088.0,\n",
       "  9412014080.0,\n",
       "  9405341696.0,\n",
       "  9316884480.0,\n",
       "  9333164032.0,\n",
       "  9353298944.0,\n",
       "  9422299136.0,\n",
       "  9417920512.0,\n",
       "  9380158464.0,\n",
       "  9311190016.0,\n",
       "  9406793728.0,\n",
       "  9318759424.0,\n",
       "  9342644224.0,\n",
       "  9424389120.0,\n",
       "  9293895680.0,\n",
       "  9326827520.0,\n",
       "  9360545792.0,\n",
       "  9339948032.0,\n",
       "  9348406272.0,\n",
       "  9436729344.0,\n",
       "  9370926080.0,\n",
       "  9406423040.0,\n",
       "  9342388224.0,\n",
       "  9347705856.0,\n",
       "  9292028928.0,\n",
       "  9407069184.0,\n",
       "  9379727360.0,\n",
       "  9351848960.0,\n",
       "  9330112512.0,\n",
       "  9247301632.0,\n",
       "  9399933952.0,\n",
       "  9245758464.0,\n",
       "  9296636928.0,\n",
       "  9648381952.0,\n",
       "  9430140928.0,\n",
       "  9220421632.0,\n",
       "  9458190336.0,\n",
       "  9279796224.0,\n",
       "  9331456000.0,\n",
       "  9302509568.0,\n",
       "  9299272704.0,\n",
       "  9334003712.0,\n",
       "  9358951424.0,\n",
       "  9437752320.0,\n",
       "  9181312000.0,\n",
       "  9188725760.0,\n",
       "  9201756160.0,\n",
       "  9439140864.0,\n",
       "  9196400640.0,\n",
       "  9215125504.0,\n",
       "  9277498368.0,\n",
       "  9576059904.0,\n",
       "  9163113472.0,\n",
       "  9247966208.0,\n",
       "  9400756224.0,\n",
       "  9362080768.0,\n",
       "  9132978176.0,\n",
       "  9271377920.0,\n",
       "  9189752832.0,\n",
       "  9131262976.0,\n",
       "  9304333312.0,\n",
       "  9246097408.0,\n",
       "  9343802368.0,\n",
       "  9192259584.0,\n",
       "  9242969088.0,\n",
       "  9264749568.0,\n",
       "  9184106496.0,\n",
       "  9347681280.0,\n",
       "  9156654080.0,\n",
       "  9278073856.0,\n",
       "  9149078528.0,\n",
       "  9204890624.0,\n",
       "  9199442944.0,\n",
       "  9197992960.0,\n",
       "  9206611968.0,\n",
       "  9181585408.0,\n",
       "  9210921984.0,\n",
       "  9214341120.0,\n",
       "  9180221440.0,\n",
       "  9377477632.0,\n",
       "  9175588864.0,\n",
       "  9233249280.0,\n",
       "  9152902144.0,\n",
       "  9163943936.0,\n",
       "  9270258688.0,\n",
       "  9315543040.0,\n",
       "  9404004352.0,\n",
       "  9176510464.0,\n",
       "  9328239616.0,\n",
       "  9163098112.0,\n",
       "  9126384640.0,\n",
       "  9199876096.0,\n",
       "  9080158208.0,\n",
       "  9171101696.0,\n",
       "  9136744448.0,\n",
       "  9090444288.0,\n",
       "  9168843776.0,\n",
       "  9249132544.0,\n",
       "  9179454464.0,\n",
       "  9182594048.0,\n",
       "  9240018944.0,\n",
       "  9108204544.0,\n",
       "  9162171392.0,\n",
       "  9197850624.0,\n",
       "  9144026112.0,\n",
       "  9127607296.0,\n",
       "  9235952640.0,\n",
       "  9374984192.0,\n",
       "  9063304192.0,\n",
       "  9119626240.0,\n",
       "  9186561024.0,\n",
       "  9119158272.0,\n",
       "  9031864320.0,\n",
       "  9088966656.0,\n",
       "  9109979136.0,\n",
       "  9105712128.0,\n",
       "  9117190144.0,\n",
       "  9056096256.0,\n",
       "  9132972032.0,\n",
       "  9022874624.0,\n",
       "  9110545408.0,\n",
       "  9035295744.0,\n",
       "  9112998912.0,\n",
       "  9130911744.0,\n",
       "  9092993024.0,\n",
       "  9196198912.0,\n",
       "  9075320832.0,\n",
       "  9173922816.0,\n",
       "  9077020672.0,\n",
       "  9039898624.0,\n",
       "  9094769664.0,\n",
       "  9204987904.0,\n",
       "  9021066240.0,\n",
       "  9233556480.0,\n",
       "  9213518848.0,\n",
       "  9121906688.0,\n",
       "  8997108736.0,\n",
       "  9000123392.0,\n",
       "  9036787712.0,\n",
       "  9031514112.0,\n",
       "  9062530048.0,\n",
       "  9089627136.0,\n",
       "  9027218432.0,\n",
       "  8966860800.0,\n",
       "  9004277760.0,\n",
       "  8971233280.0,\n",
       "  9140848640.0,\n",
       "  9086220288.0,\n",
       "  9338261504.0,\n",
       "  8968952832.0,\n",
       "  9139039232.0,\n",
       "  8939260928.0,\n",
       "  8909920256.0,\n",
       "  9029184512.0,\n",
       "  9116626944.0,\n",
       "  9057915904.0,\n",
       "  8958238720.0,\n",
       "  9076541440.0,\n",
       "  8954636288.0,\n",
       "  9154118656.0,\n",
       "  8943304704.0,\n",
       "  9130469376.0,\n",
       "  8893985792.0,\n",
       "  8913609728.0,\n",
       "  8984491008.0,\n",
       "  9168728064.0,\n",
       "  8981567488.0,\n",
       "  9094730752.0,\n",
       "  9172954112.0,\n",
       "  8917914624.0,\n",
       "  8967326720.0,\n",
       "  8928151552.0,\n",
       "  9171028992.0,\n",
       "  9002697728.0,\n",
       "  8983932928.0,\n",
       "  8960066560.0,\n",
       "  8926481408.0,\n",
       "  8928841728.0,\n",
       "  8941496320.0,\n",
       "  8908953600.0,\n",
       "  8976504832.0,\n",
       "  8982089728.0,\n",
       "  8989127680.0,\n",
       "  8955979776.0,\n",
       "  9020158976.0,\n",
       "  8952762368.0,\n",
       "  8867247104.0,\n",
       "  8905722880.0,\n",
       "  8920049664.0,\n",
       "  8808820736.0,\n",
       "  8918463488.0,\n",
       "  9017250816.0,\n",
       "  8987014144.0,\n",
       "  8839085056.0,\n",
       "  8916721664.0,\n",
       "  8856772608.0,\n",
       "  8809882624.0,\n",
       "  8850664448.0,\n",
       "  9061869568.0,\n",
       "  8867240960.0,\n",
       "  8886548480.0,\n",
       "  8826963968.0,\n",
       "  8916056064.0,\n",
       "  8800753664.0,\n",
       "  8812814336.0,\n",
       "  8818030592.0,\n",
       "  8976257024.0,\n",
       "  8759723008.0,\n",
       "  8831867904.0,\n",
       "  8781282304.0,\n",
       "  8851105792.0,\n",
       "  8827648000.0,\n",
       "  8817437696.0,\n",
       "  8848032768.0,\n",
       "  8735526912.0,\n",
       "  8900378624.0,\n",
       "  8840478720.0,\n",
       "  9112067072.0,\n",
       "  8896066560.0,\n",
       "  8850993152.0,\n",
       "  8742233088.0,\n",
       "  8982360064.0,\n",
       "  8813216768.0,\n",
       "  8938203136.0,\n",
       "  8810303488.0,\n",
       "  8759646208.0,\n",
       "  8780316672.0,\n",
       "  8922574848.0,\n",
       "  8829805568.0,\n",
       "  8707224576.0,\n",
       "  8724895744.0,\n",
       "  8721434624.0,\n",
       "  8945988608.0,\n",
       "  8796323840.0,\n",
       "  8893549568.0,\n",
       "  8689358848.0,\n",
       "  8768494592.0,\n",
       "  8733415424.0,\n",
       "  8915342336.0,\n",
       "  8842519552.0,\n",
       "  8832753664.0,\n",
       "  8733454336.0,\n",
       "  8712196096.0,\n",
       "  8729155584.0,\n",
       "  8769897472.0,\n",
       "  8850985984.0,\n",
       "  8783555584.0,\n",
       "  8757563392.0,\n",
       "  8721210368.0,\n",
       "  8736409600.0,\n",
       "  8766257152.0,\n",
       "  8695633920.0,\n",
       "  8767978496.0,\n",
       "  8766600192.0,\n",
       "  8819206144.0,\n",
       "  8690437120.0,\n",
       "  8922756096.0,\n",
       "  8764310528.0,\n",
       "  8609692672.0,\n",
       "  8892552192.0,\n",
       "  8614635520.0,\n",
       "  8775163904.0,\n",
       "  8681186304.0,\n",
       "  8670203904.0,\n",
       "  8628741120.0,\n",
       "  8780550144.0,\n",
       "  8677213184.0,\n",
       "  8689521664.0,\n",
       "  8731930624.0,\n",
       "  8755789824.0,\n",
       "  8683258880.0,\n",
       "  8717588480.0,\n",
       "  8694680576.0,\n",
       "  8621857792.0,\n",
       "  8687195136.0,\n",
       "  8539878912.0,\n",
       "  8697328640.0,\n",
       "  8680393728.0,\n",
       "  8533033472.0,\n",
       "  8512486400.0,\n",
       "  8831543296.0,\n",
       "  8850014208.0,\n",
       "  8626978816.0,\n",
       "  8558933504.0,\n",
       "  8601466880.0,\n",
       "  8559651328.0,\n",
       "  8612725760.0,\n",
       "  8692404224.0,\n",
       "  8533766656.0,\n",
       "  8901016576.0,\n",
       "  8519130624.0,\n",
       "  8590620672.0,\n",
       "  8683220992.0,\n",
       "  8563466752.0,\n",
       "  8614883328.0,\n",
       "  8669927424.0,\n",
       "  8550153728.0,\n",
       "  8665283584.0,\n",
       "  8890966016.0,\n",
       "  8689235968.0,\n",
       "  8552796160.0,\n",
       "  8636908544.0,\n",
       "  8487652352.0,\n",
       "  8707689472.0,\n",
       "  8534570496.0,\n",
       "  8536993792.0,\n",
       "  8529931776.0,\n",
       "  8496797696.0,\n",
       "  8540750336.0,\n",
       "  8544974848.0,\n",
       "  8473165824.0,\n",
       "  8722605056.0,\n",
       "  8473405952.0,\n",
       "  8524574720.0,\n",
       "  8483581440.0,\n",
       "  8604655616.0,\n",
       "  8555628032.0,\n",
       "  8492612608.0,\n",
       "  8676304896.0,\n",
       "  8495360000.0,\n",
       "  8632982528.0,\n",
       "  8517615616.0,\n",
       "  8694079488.0,\n",
       "  8609116160.0,\n",
       "  8555848704.0,\n",
       "  8445071360.0,\n",
       "  8578951680.0,\n",
       "  8619727872.0,\n",
       "  8639833088.0,\n",
       "  8515043328.0,\n",
       "  8642629632.0,\n",
       "  8459813888.0,\n",
       "  8374190080.0,\n",
       "  8429605888.0,\n",
       "  8526987264.0,\n",
       "  8497071104.0,\n",
       "  8503455232.0,\n",
       "  8387655168.0,\n",
       "  8535204352.0,\n",
       "  8467588608.0,\n",
       "  8526819328.0,\n",
       "  8544565248.0,\n",
       "  8458930688.0,\n",
       "  8553231360.0,\n",
       "  8565581824.0,\n",
       "  8509133824.0,\n",
       "  8788077568.0,\n",
       "  8599003136.0,\n",
       "  8494499328.0,\n",
       "  8458165248.0,\n",
       "  8513511424.0,\n",
       "  8625100800.0,\n",
       "  8567608320.0,\n",
       "  8384774656.0,\n",
       "  8660715520.0,\n",
       "  8465075200.0,\n",
       "  8477708288.0,\n",
       "  8354923008.0,\n",
       "  8456728576.0,\n",
       "  8396842496.0,\n",
       "  8414985728.0,\n",
       "  8803794944.0,\n",
       "  8400645120.0,\n",
       "  8539796480.0,\n",
       "  8435547136.0,\n",
       "  8557869056.0,\n",
       "  8416454656.0,\n",
       "  8444816896.0,\n",
       "  8553241088.0,\n",
       "  8393757184.0,\n",
       "  8632897536.0,\n",
       "  8413848064.0,\n",
       "  8396255744.0,\n",
       "  8385241088.0,\n",
       "  8434413056.0,\n",
       "  8593830912.0,\n",
       "  8465068032.0,\n",
       "  8423589888.0,\n",
       "  8302309888.0,\n",
       "  8379443200.0,\n",
       "  8456816640.0,\n",
       "  8393431040.0,\n",
       "  8422281728.0,\n",
       "  8600736768.0,\n",
       "  8397868032.0,\n",
       "  8383792640.0,\n",
       "  8381372416.0,\n",
       "  8419368448.0,\n",
       "  8400529408.0,\n",
       "  8423102976.0,\n",
       "  8427252224.0,\n",
       "  8530206208.0,\n",
       "  8437189120.0,\n",
       "  8384794112.0,\n",
       "  8337984512.0,\n",
       "  8425200640.0,\n",
       "  8326176768.0,\n",
       "  8394196480.0,\n",
       "  8346822144.0,\n",
       "  8278992384.0,\n",
       "  8348759552.0,\n",
       "  8323232256.0,\n",
       "  8433580032.0,\n",
       "  8462404608.0,\n",
       "  8608173056.0,\n",
       "  8335591936.0,\n",
       "  8449820160.0,\n",
       "  8290200064.0,\n",
       "  8376850944.0,\n",
       "  8291002368.0,\n",
       "  8292121600.0,\n",
       "  8491171840.0,\n",
       "  8520496128.0,\n",
       "  8433826816.0,\n",
       "  8616343552.0,\n",
       "  8267173888.0,\n",
       "  8390775296.0,\n",
       "  8528242688.0,\n",
       "  8420271104.0,\n",
       "  8473072128.0,\n",
       "  8272220672.0,\n",
       "  8393432064.0,\n",
       "  8463316992.0,\n",
       "  8474240512.0,\n",
       "  8291438592.0,\n",
       "  8274975232.0,\n",
       "  8450465280.0,\n",
       "  8486392320.0,\n",
       "  8311511552.0,\n",
       "  8497907200.0,\n",
       "  8353130496.0,\n",
       "  8220358656.0,\n",
       "  8417639936.0,\n",
       "  8407272960.0,\n",
       "  8423972352.0,\n",
       "  8285635072.0,\n",
       "  8164056064.0,\n",
       "  8441414144.0,\n",
       "  8380509696.0,\n",
       "  8241844736.0,\n",
       "  8205266944.0,\n",
       "  8308263424.0,\n",
       "  8275483648.0,\n",
       "  8353798144.0,\n",
       "  8417200640.0,\n",
       "  8291641856.0,\n",
       "  8207170560.0,\n",
       "  8396205568.0,\n",
       "  8248930304.0,\n",
       "  8325185024.0,\n",
       "  8536538112.0,\n",
       "  8244721664.0,\n",
       "  8151362048.0,\n",
       "  8165106688.0,\n",
       "  8172931072.0,\n",
       "  8312482304.0,\n",
       "  8335990272.0,\n",
       "  8228779008.0,\n",
       "  8260841472.0,\n",
       "  8334680064.0,\n",
       "  8261183488.0,\n",
       "  8175708672.0,\n",
       "  8317154816.0,\n",
       "  8184590848.0,\n",
       "  8312791040.0,\n",
       "  8745531392.0,\n",
       "  8240355840.0,\n",
       "  8348502528.0,\n",
       "  8228384256.0,\n",
       "  8231112704.0,\n",
       "  8487974912.0,\n",
       "  8237648384.0,\n",
       "  8155988992.0,\n",
       "  8193437184.0,\n",
       "  8318268416.0,\n",
       "  8338547200.0,\n",
       "  8456261632.0,\n",
       "  8146843136.0,\n",
       "  8252720128.0,\n",
       "  8462011904.0,\n",
       "  8189071360.0,\n",
       "  8352208384.0,\n",
       "  8342105600.0,\n",
       "  8262744064.0,\n",
       "  8118672384.0,\n",
       "  8152966144.0,\n",
       "  8354080256.0,\n",
       "  8322490880.0,\n",
       "  8213230592.0,\n",
       "  8192069120.0,\n",
       "  8195951104.0,\n",
       "  8325025280.0,\n",
       "  8212636672.0,\n",
       "  8207983104.0,\n",
       "  8235627008.0,\n",
       "  8237361664.0,\n",
       "  8131544064.0,\n",
       "  8251483136.0,\n",
       "  8114968064.0,\n",
       "  8219230720.0,\n",
       "  8305032704.0,\n",
       "  8219439616.0,\n",
       "  8113638400.0,\n",
       "  8180335104.0,\n",
       "  8139174912.0,\n",
       "  8160998912.0,\n",
       "  8291189760.0,\n",
       "  8209279488.0,\n",
       "  8237173760.0,\n",
       "  8112385536.0,\n",
       "  8100715520.0,\n",
       "  8126910976.0,\n",
       "  8142250496.0,\n",
       "  8450194432.0,\n",
       "  8169160704.0,\n",
       "  8059531776.0,\n",
       "  8171813888.0,\n",
       "  8208649728.0,\n",
       "  8243632128.0,\n",
       "  8244769280.0,\n",
       "  8260738560.0,\n",
       "  8233497600.0,\n",
       "  8341284352.0,\n",
       "  8454273024.0,\n",
       "  8141656576.0,\n",
       "  8132645376.0,\n",
       "  8156160000.0,\n",
       "  8072457728.0,\n",
       "  8146443264.0,\n",
       "  8115664896.0,\n",
       "  8060733440.0,\n",
       "  8143394816.0,\n",
       "  8230774784.0,\n",
       "  8122716160.0,\n",
       "  8051563008.0,\n",
       "  8190129664.0,\n",
       "  8282574336.0,\n",
       "  8090481152.0,\n",
       "  8175265792.0,\n",
       "  8136087552.0,\n",
       "  8150169088.0,\n",
       "  8113043456.0,\n",
       "  8261454848.0,\n",
       "  8036549120.0,\n",
       "  8273209856.0,\n",
       "  8178826752.0,\n",
       "  8186414080.0,\n",
       "  8038707712.0,\n",
       "  8099417088.0,\n",
       "  8200987136.0,\n",
       "  8067737088.0,\n",
       "  8106367488.0,\n",
       "  8123611136.0,\n",
       "  8211604992.0,\n",
       "  8174299136.0,\n",
       "  8139538944.0,\n",
       "  8132252672.0,\n",
       "  8086233600.0,\n",
       "  8149353984.0,\n",
       "  8193760768.0,\n",
       "  8159963136.0,\n",
       "  8084492288.0,\n",
       "  8220461056.0,\n",
       "  8030771200.0,\n",
       "  8071636480.0,\n",
       "  8099158016.0,\n",
       "  8037794304.0,\n",
       "  8222227968.0,\n",
       "  8108625408.0,\n",
       "  8224582144.0,\n",
       "  8398164480.0,\n",
       "  8072750080.0,\n",
       "  8121578496.0,\n",
       "  8211763712.0,\n",
       "  8269859840.0,\n",
       "  8000945152.0,\n",
       "  7995684352.0,\n",
       "  8178487808.0,\n",
       "  8113039872.0,\n",
       "  8060033024.0,\n",
       "  8103098880.0,\n",
       "  8104908800.0,\n",
       "  8015970816.0,\n",
       "  8176100864.0,\n",
       "  8318460928.0,\n",
       "  8062448128.0,\n",
       "  8217414144.0,\n",
       "  8059242496.0,\n",
       "  8180487680.0,\n",
       "  8109477888.0,\n",
       "  8504931840.0,\n",
       "  8066925568.0,\n",
       "  8163483648.0,\n",
       "  8080787456.0,\n",
       "  8121871872.0,\n",
       "  8173619712.0,\n",
       "  8131797504.0,\n",
       "  8013970944.0,\n",
       "  8251324928.0,\n",
       "  8072334336.0,\n",
       "  8174953472.0,\n",
       "  8030943232.0,\n",
       "  8050207744.0,\n",
       "  8013883392.0,\n",
       "  8250144256.0,\n",
       "  8104988160.0,\n",
       "  8063129088.0,\n",
       "  8102208000.0,\n",
       "  7978496512.0,\n",
       "  8085841408.0,\n",
       "  7952537600.0,\n",
       "  8063547904.0,\n",
       "  8118797824.0,\n",
       "  8046587392.0,\n",
       "  8009960960.0,\n",
       "  8065358848.0,\n",
       "  8062974464.0,\n",
       "  8067070464.0,\n",
       "  8141456896.0,\n",
       "  8028520960.0,\n",
       "  8065734144.0,\n",
       "  8159063040.0,\n",
       "  8063212544.0,\n",
       "  8128382464.0,\n",
       "  8115290112.0,\n",
       "  8019938816.0,\n",
       "  8082861568.0,\n",
       "  8092465152.0,\n",
       "  8098937856.0,\n",
       "  8202397696.0,\n",
       "  8095049728.0,\n",
       "  8143357440.0,\n",
       "  8083811328.0,\n",
       "  8054468096.0,\n",
       "  8131824128.0,\n",
       "  8148114944.0,\n",
       "  8049014784.0,\n",
       "  8134492672.0,\n",
       "  7998066176.0,\n",
       "  8098786816.0,\n",
       "  7975204352.0,\n",
       "  8071856640.0,\n",
       "  8207601152.0,\n",
       "  7986703360.0,\n",
       "  8007677952.0,\n",
       "  7913495552.0,\n",
       "  7977913856.0,\n",
       "  7979033600.0,\n",
       "  8057073152.0,\n",
       "  7982519808.0,\n",
       "  8179188224.0,\n",
       "  8044289024.0,\n",
       "  7957255168.0,\n",
       "  8155334144.0,\n",
       "  7961774080.0,\n",
       "  8096374272.0,\n",
       "  7921854464.0,\n",
       "  8270330368.0,\n",
       "  7950662656.0,\n",
       "  8023610368.0,\n",
       "  8114482688.0,\n",
       "  8097602048.0,\n",
       "  8061018112.0,\n",
       "  8059449856.0,\n",
       "  7973883904.0,\n",
       "  8235201536.0,\n",
       "  8008085504.0,\n",
       "  7955396096.0,\n",
       "  8233156608.0,\n",
       "  8087714304.0,\n",
       "  8197729792.0,\n",
       "  7941991424.0,\n",
       "  7906369024.0,\n",
       "  7996498432.0,\n",
       "  8270537728.0,\n",
       "  8014278656.0,\n",
       "  7971933184.0,\n",
       "  7987863040.0,\n",
       "  7909085696.0,\n",
       "  8064598016.0,\n",
       "  8037301248.0,\n",
       "  8241711104.0,\n",
       "  8001275904.0,\n",
       "  7993869312.0,\n",
       "  8123084288.0,\n",
       "  8024256512.0,\n",
       "  8020158976.0,\n",
       "  8027149312.0,\n",
       "  8179529216.0,\n",
       "  8333078016.0,\n",
       "  7956763648.0,\n",
       "  ...],\n",
       " 'val_loss': [121651748864.0,\n",
       "  33805815808.0,\n",
       "  33421533184.0,\n",
       "  33253955584.0,\n",
       "  32815079424.0,\n",
       "  32261232640.0,\n",
       "  31884945408.0,\n",
       "  31533219840.0,\n",
       "  31152447488.0,\n",
       "  30834788352.0,\n",
       "  30478850048.0,\n",
       "  30176585728.0,\n",
       "  29864726528.0,\n",
       "  29606297600.0,\n",
       "  29347041280.0,\n",
       "  29232973824.0,\n",
       "  28984731648.0,\n",
       "  28848052224.0,\n",
       "  28751964160.0,\n",
       "  28785172480.0,\n",
       "  28662820864.0,\n",
       "  28535869440.0,\n",
       "  28490565632.0,\n",
       "  28433197056.0,\n",
       "  28394274816.0,\n",
       "  28370284544.0,\n",
       "  28319850496.0,\n",
       "  28330700800.0,\n",
       "  28284749824.0,\n",
       "  28423077888.0,\n",
       "  28226045952.0,\n",
       "  28135555072.0,\n",
       "  28023494656.0,\n",
       "  27968542720.0,\n",
       "  27869542400.0,\n",
       "  27864537088.0,\n",
       "  27744651264.0,\n",
       "  27680800768.0,\n",
       "  27585329152.0,\n",
       "  27520360448.0,\n",
       "  27429120000.0,\n",
       "  27291074560.0,\n",
       "  27238598656.0,\n",
       "  27101986816.0,\n",
       "  26925412352.0,\n",
       "  26761228288.0,\n",
       "  26605821952.0,\n",
       "  26376333312.0,\n",
       "  26289717248.0,\n",
       "  25907394560.0,\n",
       "  25806309376.0,\n",
       "  25296361472.0,\n",
       "  25595908096.0,\n",
       "  24484042752.0,\n",
       "  23981008896.0,\n",
       "  23424505856.0,\n",
       "  22813712384.0,\n",
       "  22264899584.0,\n",
       "  21771671552.0,\n",
       "  21091950592.0,\n",
       "  20479369216.0,\n",
       "  20749963264.0,\n",
       "  20034146304.0,\n",
       "  19426631680.0,\n",
       "  19055863808.0,\n",
       "  19007180800.0,\n",
       "  18700388352.0,\n",
       "  18289217536.0,\n",
       "  18006878208.0,\n",
       "  17776052224.0,\n",
       "  19382743040.0,\n",
       "  17323378688.0,\n",
       "  17158506496.0,\n",
       "  16990753792.0,\n",
       "  16718500864.0,\n",
       "  16812966912.0,\n",
       "  16254916608.0,\n",
       "  16044211200.0,\n",
       "  17358915584.0,\n",
       "  15799670784.0,\n",
       "  15510873088.0,\n",
       "  15390971904.0,\n",
       "  15197855744.0,\n",
       "  14922487808.0,\n",
       "  14735742976.0,\n",
       "  14583908352.0,\n",
       "  14448053248.0,\n",
       "  14255993856.0,\n",
       "  14476048384.0,\n",
       "  15039618048.0,\n",
       "  13939848192.0,\n",
       "  14775622656.0,\n",
       "  13594251264.0,\n",
       "  13476077568.0,\n",
       "  13681738752.0,\n",
       "  13228256256.0,\n",
       "  13278540800.0,\n",
       "  13303195648.0,\n",
       "  13243542528.0,\n",
       "  12947309568.0,\n",
       "  12852815872.0,\n",
       "  12661199872.0,\n",
       "  12554247168.0,\n",
       "  12670345216.0,\n",
       "  12426853376.0,\n",
       "  12562331648.0,\n",
       "  12290693120.0,\n",
       "  12213680128.0,\n",
       "  13427531776.0,\n",
       "  12275852288.0,\n",
       "  12100200448.0,\n",
       "  12811540480.0,\n",
       "  12163513344.0,\n",
       "  11899004928.0,\n",
       "  11942779904.0,\n",
       "  11818808320.0,\n",
       "  11834753024.0,\n",
       "  12048612352.0,\n",
       "  12673475584.0,\n",
       "  12121057280.0,\n",
       "  11667452928.0,\n",
       "  11915550720.0,\n",
       "  12179184640.0,\n",
       "  11662941184.0,\n",
       "  11485127680.0,\n",
       "  11706031104.0,\n",
       "  11402972160.0,\n",
       "  11452746752.0,\n",
       "  11455851520.0,\n",
       "  11401339904.0,\n",
       "  11279265792.0,\n",
       "  11487184896.0,\n",
       "  11501602816.0,\n",
       "  11296930816.0,\n",
       "  11177009152.0,\n",
       "  11143933952.0,\n",
       "  11453154304.0,\n",
       "  11229430784.0,\n",
       "  11108550656.0,\n",
       "  11069937664.0,\n",
       "  11321811968.0,\n",
       "  11052755968.0,\n",
       "  11062890496.0,\n",
       "  10957779968.0,\n",
       "  10959354880.0,\n",
       "  10930849792.0,\n",
       "  10898952192.0,\n",
       "  10883216384.0,\n",
       "  10899063808.0,\n",
       "  10930805760.0,\n",
       "  10903649280.0,\n",
       "  10814370816.0,\n",
       "  11037131776.0,\n",
       "  10971823104.0,\n",
       "  10798851072.0,\n",
       "  10866807808.0,\n",
       "  11157164032.0,\n",
       "  10805097472.0,\n",
       "  11087403008.0,\n",
       "  10716851200.0,\n",
       "  10680773632.0,\n",
       "  10736720896.0,\n",
       "  10989752320.0,\n",
       "  10922390528.0,\n",
       "  10680645632.0,\n",
       "  12116536320.0,\n",
       "  10674270208.0,\n",
       "  10657203200.0,\n",
       "  10548128768.0,\n",
       "  10697455616.0,\n",
       "  10573127680.0,\n",
       "  10894712832.0,\n",
       "  10502206464.0,\n",
       "  11243939840.0,\n",
       "  10476268544.0,\n",
       "  10487940096.0,\n",
       "  10459224064.0,\n",
       "  10840945664.0,\n",
       "  10533189632.0,\n",
       "  10542206976.0,\n",
       "  10479326208.0,\n",
       "  10408235008.0,\n",
       "  10457877504.0,\n",
       "  11121180672.0,\n",
       "  10661410816.0,\n",
       "  10493634560.0,\n",
       "  10778881024.0,\n",
       "  10354818048.0,\n",
       "  10352636928.0,\n",
       "  10324648960.0,\n",
       "  10313592832.0,\n",
       "  10352257024.0,\n",
       "  11420983296.0,\n",
       "  12064497664.0,\n",
       "  10326672384.0,\n",
       "  11077406720.0,\n",
       "  10392454144.0,\n",
       "  10331490304.0,\n",
       "  10255128576.0,\n",
       "  10360438784.0,\n",
       "  10445952000.0,\n",
       "  10322898944.0,\n",
       "  10328345600.0,\n",
       "  10217179136.0,\n",
       "  10224529408.0,\n",
       "  10249733120.0,\n",
       "  10513784832.0,\n",
       "  10268438528.0,\n",
       "  10248075264.0,\n",
       "  10546381824.0,\n",
       "  10524796928.0,\n",
       "  11280625664.0,\n",
       "  10181732352.0,\n",
       "  10331469824.0,\n",
       "  10182817792.0,\n",
       "  10215666688.0,\n",
       "  10160473088.0,\n",
       "  10118451200.0,\n",
       "  10124310528.0,\n",
       "  10258376704.0,\n",
       "  10271807488.0,\n",
       "  10123487232.0,\n",
       "  10347310080.0,\n",
       "  10597818368.0,\n",
       "  10680974336.0,\n",
       "  10153350144.0,\n",
       "  10259228672.0,\n",
       "  10281807872.0,\n",
       "  12059142144.0,\n",
       "  10515691520.0,\n",
       "  10076860416.0,\n",
       "  10418735104.0,\n",
       "  10135532544.0,\n",
       "  10030074880.0,\n",
       "  10028502016.0,\n",
       "  10022706176.0,\n",
       "  10122637312.0,\n",
       "  10121010176.0,\n",
       "  10010890240.0,\n",
       "  10013418496.0,\n",
       "  10278811648.0,\n",
       "  10019051520.0,\n",
       "  10060568576.0,\n",
       "  9978777600.0,\n",
       "  10099008512.0,\n",
       "  10028488704.0,\n",
       "  10018449408.0,\n",
       "  10066475008.0,\n",
       "  10056970240.0,\n",
       "  9951105024.0,\n",
       "  10034532352.0,\n",
       "  10065807360.0,\n",
       "  10101623808.0,\n",
       "  9987835904.0,\n",
       "  9994050560.0,\n",
       "  10065167360.0,\n",
       "  9943063552.0,\n",
       "  10079106048.0,\n",
       "  10322444288.0,\n",
       "  10154254336.0,\n",
       "  9998039040.0,\n",
       "  9999311872.0,\n",
       "  9916487680.0,\n",
       "  9901035520.0,\n",
       "  10041817088.0,\n",
       "  9923571712.0,\n",
       "  9899122688.0,\n",
       "  10444371968.0,\n",
       "  9881648128.0,\n",
       "  10639885312.0,\n",
       "  10097820672.0,\n",
       "  9983564800.0,\n",
       "  9888453632.0,\n",
       "  9909404672.0,\n",
       "  9869542400.0,\n",
       "  9969035264.0,\n",
       "  10106379264.0,\n",
       "  10105925632.0,\n",
       "  10093821952.0,\n",
       "  10240599040.0,\n",
       "  10219981824.0,\n",
       "  9857268736.0,\n",
       "  10583835648.0,\n",
       "  10719534080.0,\n",
       "  9829543936.0,\n",
       "  9935612928.0,\n",
       "  10267525120.0,\n",
       "  9877794816.0,\n",
       "  10005691392.0,\n",
       "  9812784128.0,\n",
       "  9819501568.0,\n",
       "  10216805376.0,\n",
       "  9863550976.0,\n",
       "  9955622912.0,\n",
       "  9864024064.0,\n",
       "  9974926336.0,\n",
       "  9890370560.0,\n",
       "  9907855360.0,\n",
       "  9851830272.0,\n",
       "  9951016960.0,\n",
       "  9804910592.0,\n",
       "  9775299584.0,\n",
       "  9818556416.0,\n",
       "  9772422144.0,\n",
       "  9774658560.0,\n",
       "  9829755904.0,\n",
       "  9940729856.0,\n",
       "  9949035520.0,\n",
       "  9860573184.0,\n",
       "  10135888896.0,\n",
       "  9819897856.0,\n",
       "  9758238720.0,\n",
       "  9884778496.0,\n",
       "  10245359616.0,\n",
       "  10337086464.0,\n",
       "  9803785216.0,\n",
       "  10416309248.0,\n",
       "  9757053952.0,\n",
       "  9852983296.0,\n",
       "  9922259968.0,\n",
       "  10809008128.0,\n",
       "  9713469440.0,\n",
       "  9743801344.0,\n",
       "  9715138560.0,\n",
       "  9926137856.0,\n",
       "  10077670400.0,\n",
       "  9845243904.0,\n",
       "  9844768768.0,\n",
       "  10482500608.0,\n",
       "  10759534592.0,\n",
       "  10003842048.0,\n",
       "  11330939904.0,\n",
       "  10163679232.0,\n",
       "  10147943424.0,\n",
       "  9707617280.0,\n",
       "  9971558400.0,\n",
       "  9777119232.0,\n",
       "  9742564352.0,\n",
       "  10152089600.0,\n",
       "  10354249728.0,\n",
       "  10445366272.0,\n",
       "  9655609344.0,\n",
       "  9874927616.0,\n",
       "  9770396672.0,\n",
       "  9689684992.0,\n",
       "  9741605888.0,\n",
       "  9668117504.0,\n",
       "  10372404224.0,\n",
       "  10052690944.0,\n",
       "  9700419584.0,\n",
       "  9856845824.0,\n",
       "  9736910848.0,\n",
       "  9802020864.0,\n",
       "  9780618240.0,\n",
       "  9650753536.0,\n",
       "  9626433536.0,\n",
       "  10077400064.0,\n",
       "  9630563328.0,\n",
       "  9960851456.0,\n",
       "  9756936192.0,\n",
       "  9610330112.0,\n",
       "  9749440512.0,\n",
       "  9691880448.0,\n",
       "  9727995904.0,\n",
       "  9946019840.0,\n",
       "  9647207424.0,\n",
       "  9946467328.0,\n",
       "  9940835328.0,\n",
       "  9629674496.0,\n",
       "  9868680192.0,\n",
       "  9703033856.0,\n",
       "  9842055168.0,\n",
       "  10032323584.0,\n",
       "  11215206400.0,\n",
       "  9578017792.0,\n",
       "  9803070464.0,\n",
       "  9590699008.0,\n",
       "  9601048576.0,\n",
       "  9827960832.0,\n",
       "  9763018752.0,\n",
       "  9545158656.0,\n",
       "  9640846336.0,\n",
       "  9556244480.0,\n",
       "  10452763648.0,\n",
       "  9655697408.0,\n",
       "  9564323840.0,\n",
       "  9926744064.0,\n",
       "  9536334848.0,\n",
       "  9618859008.0,\n",
       "  9668290560.0,\n",
       "  9583888384.0,\n",
       "  9696705536.0,\n",
       "  9512406016.0,\n",
       "  10088907776.0,\n",
       "  10059883520.0,\n",
       "  9623529472.0,\n",
       "  9494210560.0,\n",
       "  9774370816.0,\n",
       "  9500983296.0,\n",
       "  10352370688.0,\n",
       "  9549344768.0,\n",
       "  9590152192.0,\n",
       "  9494789120.0,\n",
       "  9564019712.0,\n",
       "  9903196160.0,\n",
       "  9501195264.0,\n",
       "  9496472576.0,\n",
       "  9822010368.0,\n",
       "  9515010048.0,\n",
       "  9484250112.0,\n",
       "  12175668224.0,\n",
       "  9461998592.0,\n",
       "  9816753152.0,\n",
       "  9452861440.0,\n",
       "  9600391168.0,\n",
       "  10201759744.0,\n",
       "  9449287680.0,\n",
       "  9578489856.0,\n",
       "  11155677184.0,\n",
       "  9888029696.0,\n",
       "  9640536064.0,\n",
       "  9421490176.0,\n",
       "  9435419648.0,\n",
       "  9781048320.0,\n",
       "  9504129024.0,\n",
       "  9589169152.0,\n",
       "  9579359232.0,\n",
       "  9430033408.0,\n",
       "  9434058752.0,\n",
       "  9673759744.0,\n",
       "  9396048896.0,\n",
       "  9533064192.0,\n",
       "  9419074560.0,\n",
       "  9429105664.0,\n",
       "  9992867840.0,\n",
       "  9807170560.0,\n",
       "  9518040064.0,\n",
       "  9415535616.0,\n",
       "  9786553344.0,\n",
       "  9995325440.0,\n",
       "  9399278592.0,\n",
       "  9380713472.0,\n",
       "  9421586432.0,\n",
       "  9402184704.0,\n",
       "  9412696064.0,\n",
       "  9386516480.0,\n",
       "  9408462848.0,\n",
       "  9964130304.0,\n",
       "  9358072832.0,\n",
       "  10032101376.0,\n",
       "  9424875520.0,\n",
       "  9449362432.0,\n",
       "  9403250688.0,\n",
       "  9447225344.0,\n",
       "  10162466816.0,\n",
       "  10074076160.0,\n",
       "  9345093632.0,\n",
       "  9424077824.0,\n",
       "  9556606976.0,\n",
       "  9347800064.0,\n",
       "  9313346560.0,\n",
       "  9317913600.0,\n",
       "  9409338368.0,\n",
       "  9923647488.0,\n",
       "  9301094400.0,\n",
       "  11188537344.0,\n",
       "  10120796160.0,\n",
       "  9493874688.0,\n",
       "  9299046400.0,\n",
       "  9469286400.0,\n",
       "  9503165440.0,\n",
       "  9307029504.0,\n",
       "  9322513408.0,\n",
       "  9685358592.0,\n",
       "  9327153152.0,\n",
       "  9309953024.0,\n",
       "  9809286144.0,\n",
       "  9432254464.0,\n",
       "  9252619264.0,\n",
       "  9252713472.0,\n",
       "  9769564160.0,\n",
       "  9285260288.0,\n",
       "  9748521984.0,\n",
       "  9541941248.0,\n",
       "  9724964864.0,\n",
       "  9340605440.0,\n",
       "  9322785792.0,\n",
       "  9533277184.0,\n",
       "  9216871424.0,\n",
       "  9317487616.0,\n",
       "  9293391872.0,\n",
       "  9223870464.0,\n",
       "  9603372032.0,\n",
       "  9809209344.0,\n",
       "  9329930240.0,\n",
       "  9736264704.0,\n",
       "  9212895232.0,\n",
       "  9249668096.0,\n",
       "  9622026240.0,\n",
       "  9177433088.0,\n",
       "  9652167680.0,\n",
       "  9246261248.0,\n",
       "  9457838080.0,\n",
       "  9312258048.0,\n",
       "  9595968512.0,\n",
       "  9159065600.0,\n",
       "  9163876352.0,\n",
       "  9188305920.0,\n",
       "  9148314624.0,\n",
       "  9308774400.0,\n",
       "  9418340352.0,\n",
       "  9742518272.0,\n",
       "  9138418688.0,\n",
       "  9335789568.0,\n",
       "  9890980864.0,\n",
       "  9577356288.0,\n",
       "  9176811520.0,\n",
       "  9693710336.0,\n",
       "  9942761472.0,\n",
       "  9209896960.0,\n",
       "  9171511296.0,\n",
       "  9394874368.0,\n",
       "  9260360704.0,\n",
       "  9480943616.0,\n",
       "  9093772288.0,\n",
       "  9218597888.0,\n",
       "  9101099008.0,\n",
       "  9175414784.0,\n",
       "  9159562240.0,\n",
       "  9385996288.0,\n",
       "  9807713280.0,\n",
       "  9148485632.0,\n",
       "  9631598592.0,\n",
       "  9069587456.0,\n",
       "  9616350208.0,\n",
       "  9159624704.0,\n",
       "  9158617088.0,\n",
       "  9516555264.0,\n",
       "  9079612416.0,\n",
       "  9145381888.0,\n",
       "  9070360576.0,\n",
       "  9100327936.0,\n",
       "  9069433856.0,\n",
       "  9118898176.0,\n",
       "  9130051584.0,\n",
       "  9072428032.0,\n",
       "  9086223360.0,\n",
       "  9764575232.0,\n",
       "  9451204608.0,\n",
       "  9415933952.0,\n",
       "  9266436096.0,\n",
       "  9026357248.0,\n",
       "  9094046720.0,\n",
       "  9205821440.0,\n",
       "  9040287744.0,\n",
       "  9144380416.0,\n",
       "  9376315392.0,\n",
       "  9455513600.0,\n",
       "  9002295296.0,\n",
       "  9006089216.0,\n",
       "  9117476864.0,\n",
       "  9235776512.0,\n",
       "  9580869632.0,\n",
       "  9191960576.0,\n",
       "  9049550848.0,\n",
       "  8984992768.0,\n",
       "  10095241216.0,\n",
       "  9114564608.0,\n",
       "  9643894784.0,\n",
       "  9057583104.0,\n",
       "  10349403136.0,\n",
       "  9197110272.0,\n",
       "  9081540608.0,\n",
       "  10010828800.0,\n",
       "  8985408512.0,\n",
       "  10001006592.0,\n",
       "  9256446976.0,\n",
       "  9333738496.0,\n",
       "  8970407936.0,\n",
       "  8922594304.0,\n",
       "  9647361024.0,\n",
       "  9073434624.0,\n",
       "  9309995008.0,\n",
       "  8964470784.0,\n",
       "  9354992640.0,\n",
       "  9007283200.0,\n",
       "  8918995968.0,\n",
       "  8930482176.0,\n",
       "  10081339392.0,\n",
       "  9142334464.0,\n",
       "  9617323008.0,\n",
       "  8944274432.0,\n",
       "  8888942592.0,\n",
       "  9107525632.0,\n",
       "  8868497408.0,\n",
       "  9006855168.0,\n",
       "  9083482112.0,\n",
       "  9088117760.0,\n",
       "  9242075136.0,\n",
       "  8853453824.0,\n",
       "  8922667008.0,\n",
       "  9252147200.0,\n",
       "  8880117760.0,\n",
       "  9157443584.0,\n",
       "  8908386304.0,\n",
       "  9224430592.0,\n",
       "  8930794496.0,\n",
       "  8996158464.0,\n",
       "  8897096704.0,\n",
       "  9104295936.0,\n",
       "  8827396096.0,\n",
       "  8852554752.0,\n",
       "  9230244864.0,\n",
       "  9712400384.0,\n",
       "  8879113216.0,\n",
       "  9035448320.0,\n",
       "  8894766080.0,\n",
       "  9075643392.0,\n",
       "  9255709696.0,\n",
       "  9028657152.0,\n",
       "  8928428032.0,\n",
       "  8803575808.0,\n",
       "  9048842240.0,\n",
       "  8917595136.0,\n",
       "  8896594944.0,\n",
       "  8865838080.0,\n",
       "  9275517952.0,\n",
       "  8814867456.0,\n",
       "  9444330496.0,\n",
       "  9761799168.0,\n",
       "  8826277888.0,\n",
       "  8912933888.0,\n",
       "  8798926848.0,\n",
       "  8983969792.0,\n",
       "  8834949120.0,\n",
       "  8822287360.0,\n",
       "  9170153472.0,\n",
       "  8844010496.0,\n",
       "  8789339136.0,\n",
       "  8929703936.0,\n",
       "  9102534656.0,\n",
       "  8932526080.0,\n",
       "  8805270528.0,\n",
       "  8873345024.0,\n",
       "  8924609536.0,\n",
       "  8768906240.0,\n",
       "  8767328256.0,\n",
       "  8782252032.0,\n",
       "  8856312832.0,\n",
       "  9128896512.0,\n",
       "  8827979776.0,\n",
       "  8757969920.0,\n",
       "  9094014976.0,\n",
       "  9663540224.0,\n",
       "  8843806720.0,\n",
       "  9021180928.0,\n",
       "  8773748736.0,\n",
       "  9256414208.0,\n",
       "  8976706560.0,\n",
       "  8782522368.0,\n",
       "  9084452864.0,\n",
       "  9192629248.0,\n",
       "  9028144128.0,\n",
       "  8769465344.0,\n",
       "  10021844992.0,\n",
       "  8730780672.0,\n",
       "  8924385280.0,\n",
       "  9051868160.0,\n",
       "  8763508736.0,\n",
       "  9129175040.0,\n",
       "  9263371264.0,\n",
       "  8718182400.0,\n",
       "  9347201024.0,\n",
       "  9246791680.0,\n",
       "  8981100544.0,\n",
       "  9000353792.0,\n",
       "  8751771648.0,\n",
       "  9046367232.0,\n",
       "  9084712960.0,\n",
       "  9179156480.0,\n",
       "  8983451648.0,\n",
       "  8832472064.0,\n",
       "  9158653952.0,\n",
       "  9075487744.0,\n",
       "  9707099136.0,\n",
       "  9158317056.0,\n",
       "  8774584320.0,\n",
       "  8713754624.0,\n",
       "  8683455488.0,\n",
       "  9177980928.0,\n",
       "  8858898432.0,\n",
       "  9144964096.0,\n",
       "  9241196544.0,\n",
       "  8871353344.0,\n",
       "  9064932352.0,\n",
       "  8734075904.0,\n",
       "  9241708544.0,\n",
       "  8676783104.0,\n",
       "  8921054208.0,\n",
       "  8944054272.0,\n",
       "  8738407424.0,\n",
       "  8912646144.0,\n",
       "  8830019584.0,\n",
       "  8662644736.0,\n",
       "  8799254528.0,\n",
       "  8822984704.0,\n",
       "  8662922240.0,\n",
       "  8783020032.0,\n",
       "  8668043264.0,\n",
       "  9128446976.0,\n",
       "  8653604864.0,\n",
       "  9006274560.0,\n",
       "  8697773056.0,\n",
       "  8649455616.0,\n",
       "  8728842240.0,\n",
       "  9097964544.0,\n",
       "  8738633728.0,\n",
       "  8629012480.0,\n",
       "  9303080960.0,\n",
       "  8874224640.0,\n",
       "  9010429952.0,\n",
       "  9229598720.0,\n",
       "  9109873664.0,\n",
       "  8703131648.0,\n",
       "  8748336128.0,\n",
       "  8725636096.0,\n",
       "  8655560704.0,\n",
       "  8801123328.0,\n",
       "  8674916352.0,\n",
       "  8672596992.0,\n",
       "  8891534336.0,\n",
       "  9306933248.0,\n",
       "  8746365952.0,\n",
       "  8824502272.0,\n",
       "  8820467712.0,\n",
       "  10124879872.0,\n",
       "  8643406848.0,\n",
       "  9081523200.0,\n",
       "  10082252800.0,\n",
       "  8774900736.0,\n",
       "  8675335168.0,\n",
       "  8680804352.0,\n",
       "  8592815104.0,\n",
       "  8586430976.0,\n",
       "  8620393472.0,\n",
       "  8719977472.0,\n",
       "  8608933888.0,\n",
       "  8587520512.0,\n",
       "  8726723584.0,\n",
       "  8658585600.0,\n",
       "  8589355008.0,\n",
       "  9076819968.0,\n",
       "  8628748288.0,\n",
       "  8689920000.0,\n",
       "  8586443776.0,\n",
       "  8607818752.0,\n",
       "  8622363648.0,\n",
       "  8722400256.0,\n",
       "  8666420224.0,\n",
       "  8830129152.0,\n",
       "  8670175232.0,\n",
       "  8574524928.0,\n",
       "  8564612096.0,\n",
       "  8988719104.0,\n",
       "  8638568448.0,\n",
       "  8848724992.0,\n",
       "  8558457344.0,\n",
       "  8852285440.0,\n",
       "  8789384192.0,\n",
       "  8806454272.0,\n",
       "  9012921344.0,\n",
       "  8913977344.0,\n",
       "  8851434496.0,\n",
       "  8574110208.0,\n",
       "  8530923520.0,\n",
       "  9443674112.0,\n",
       "  8560052736.0,\n",
       "  8627759104.0,\n",
       "  8568205312.0,\n",
       "  8784027648.0,\n",
       "  8751405056.0,\n",
       "  8742323200.0,\n",
       "  8636524544.0,\n",
       "  9077916672.0,\n",
       "  9213178880.0,\n",
       "  8748961792.0,\n",
       "  8516707328.0,\n",
       "  8698011648.0,\n",
       "  8515043840.0,\n",
       "  8524755968.0,\n",
       "  9132730368.0,\n",
       "  8798911488.0,\n",
       "  8576108544.0,\n",
       "  8814548992.0,\n",
       "  8551519744.0,\n",
       "  8639931392.0,\n",
       "  8640347136.0,\n",
       "  9042941952.0,\n",
       "  8588911104.0,\n",
       "  10261172224.0,\n",
       "  8527337472.0,\n",
       "  8479752704.0,\n",
       "  8864635904.0,\n",
       "  8562284544.0,\n",
       "  8545357312.0,\n",
       "  8679312384.0,\n",
       "  8608644096.0,\n",
       "  9461344256.0,\n",
       "  8682585088.0,\n",
       "  8932204544.0,\n",
       "  8634006528.0,\n",
       "  9822770176.0,\n",
       "  8459662848.0,\n",
       "  8474260480.0,\n",
       "  10187956224.0,\n",
       "  8939375616.0,\n",
       "  8548550656.0,\n",
       "  8642855936.0,\n",
       "  8528132608.0,\n",
       "  8514994176.0,\n",
       "  8922012672.0,\n",
       "  8492844032.0,\n",
       "  8531687936.0,\n",
       "  8535053312.0,\n",
       "  8537370112.0,\n",
       "  8534428672.0,\n",
       "  8762466304.0,\n",
       "  8591722496.0,\n",
       "  8798509056.0,\n",
       "  8529901568.0,\n",
       "  9606104064.0,\n",
       "  8893221888.0,\n",
       "  10591152128.0,\n",
       "  8556400128.0,\n",
       "  8642570240.0,\n",
       "  8428099072.0,\n",
       "  8414236160.0,\n",
       "  9109493760.0,\n",
       "  9121650688.0,\n",
       "  9466089472.0,\n",
       "  8428082688.0,\n",
       "  8432716800.0,\n",
       "  10033028096.0,\n",
       "  8597818368.0,\n",
       "  8864599040.0,\n",
       "  9014790144.0,\n",
       "  8437906944.0,\n",
       "  8455233024.0,\n",
       "  8463445504.0,\n",
       "  8676415488.0,\n",
       "  8552771584.0,\n",
       "  8555684352.0,\n",
       "  8436553216.0,\n",
       "  8675348480.0,\n",
       "  8614791168.0,\n",
       "  8398278656.0,\n",
       "  8619119616.0,\n",
       "  8533178368.0,\n",
       "  8505638912.0,\n",
       "  8540919296.0,\n",
       "  8421601280.0,\n",
       "  8974912512.0,\n",
       "  8685965312.0,\n",
       "  8553796096.0,\n",
       "  8594026496.0,\n",
       "  8369021440.0,\n",
       "  8526427136.0,\n",
       "  8577814528.0,\n",
       "  9693394944.0,\n",
       "  8513550336.0,\n",
       "  8411152384.0,\n",
       "  8406305280.0,\n",
       "  8827817984.0,\n",
       "  8443744768.0,\n",
       "  8848249856.0,\n",
       "  8838593536.0,\n",
       "  8427199488.0,\n",
       "  8423254016.0,\n",
       "  8354155008.0,\n",
       "  10329080832.0,\n",
       "  8397163008.0,\n",
       "  8534496256.0,\n",
       "  8527363072.0,\n",
       "  8694064128.0,\n",
       "  9181730816.0,\n",
       "  8527209472.0,\n",
       "  8379623936.0,\n",
       "  8486206464.0,\n",
       "  8552578560.0,\n",
       "  8702308352.0,\n",
       "  8481479680.0,\n",
       "  9334905856.0,\n",
       "  9869205504.0,\n",
       "  8639364096.0,\n",
       "  8343806464.0,\n",
       "  8397730304.0,\n",
       "  8453316096.0,\n",
       "  8368032256.0,\n",
       "  9222697984.0,\n",
       "  8463908352.0,\n",
       "  8727100416.0,\n",
       "  8397758464.0,\n",
       "  8999979008.0,\n",
       "  8497303552.0,\n",
       "  8325694464.0,\n",
       "  8394985472.0,\n",
       "  8335221248.0,\n",
       "  8373065216.0,\n",
       "  9231245312.0,\n",
       "  8358144000.0,\n",
       "  8431635456.0,\n",
       "  8406788608.0,\n",
       "  8544306688.0,\n",
       "  8510283776.0,\n",
       "  8362083328.0,\n",
       "  8374532096.0,\n",
       "  8587121152.0,\n",
       "  9283289088.0,\n",
       "  8341894144.0,\n",
       "  8323589632.0,\n",
       "  11238446080.0,\n",
       "  9916264448.0,\n",
       "  8522493440.0,\n",
       "  9675404288.0,\n",
       "  9155572736.0,\n",
       "  8973864960.0,\n",
       "  9464877056.0,\n",
       "  8399095296.0,\n",
       "  8346432512.0,\n",
       "  8296876032.0,\n",
       "  8762148864.0,\n",
       "  8465379840.0,\n",
       "  8350597120.0,\n",
       "  9481058304.0,\n",
       "  8699817984.0,\n",
       "  8358240768.0,\n",
       "  8462925312.0,\n",
       "  8424403968.0,\n",
       "  8907288576.0,\n",
       "  8518334976.0,\n",
       "  8525846016.0,\n",
       "  9357324288.0,\n",
       "  8296041472.0,\n",
       "  8335629824.0,\n",
       "  8278607360.0,\n",
       "  8560067584.0,\n",
       "  8388058624.0,\n",
       "  8569167872.0,\n",
       "  8351873024.0,\n",
       "  8407541760.0,\n",
       "  8320142848.0,\n",
       "  8289657856.0,\n",
       "  8275968512.0,\n",
       "  8586762752.0,\n",
       "  8266418688.0,\n",
       "  8364194816.0,\n",
       "  8763447296.0,\n",
       "  8474786304.0,\n",
       "  8478886912.0,\n",
       "  8362096640.0,\n",
       "  8555083264.0,\n",
       "  8589062144.0,\n",
       "  8257378304.0,\n",
       "  8404720128.0,\n",
       "  8395893248.0,\n",
       "  9366491136.0,\n",
       "  8328629760.0,\n",
       "  8268871168.0,\n",
       "  8894094336.0,\n",
       "  8563847168.0,\n",
       "  8856895488.0,\n",
       "  8339300352.0,\n",
       "  8397263360.0,\n",
       "  8295115264.0,\n",
       "  8549941760.0,\n",
       "  8620446720.0,\n",
       "  8286703616.0,\n",
       "  8282988032.0,\n",
       "  8695019520.0,\n",
       "  8399234048.0,\n",
       "  8309540352.0,\n",
       "  8456349184.0,\n",
       "  8623076352.0,\n",
       "  8721385472.0,\n",
       "  8411692032.0,\n",
       "  8416587264.0,\n",
       "  8380320256.0,\n",
       "  8454086144.0,\n",
       "  8298904576.0,\n",
       "  9328831488.0,\n",
       "  8358621184.0,\n",
       "  8273734656.0,\n",
       "  8241890304.0,\n",
       "  8655622144.0,\n",
       "  8254346240.0,\n",
       "  8972062720.0,\n",
       "  8278711296.0,\n",
       "  8533676544.0,\n",
       "  10481261568.0,\n",
       "  8247710208.0,\n",
       "  ...]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:15:38.522607Z",
     "start_time": "2024-07-27T08:15:38.450995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')"
   ],
   "id": "9f92a89dcf96812e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f870448d790>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:15:38.598495Z",
     "start_time": "2024-07-27T08:15:38.523362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_f = pd.DataFrame(history.history)\n",
    "loss_f.plot()"
   ],
   "id": "2a41f962772ed600",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:15:38.600534Z",
     "start_time": "2024-07-27T08:15:38.599073Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "59edb721bc227d4f",
   "outputs": [],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
